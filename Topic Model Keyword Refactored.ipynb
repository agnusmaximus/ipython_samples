{
 "metadata": {
  "name": "",
  "signature": "sha256:88882fb1dec9023dde420dd105d9bdcd16be0bdf877c9e68b015e83fab91d1ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# topic_iterator.py\n",
      "\n",
      "import scipy.sparse\n",
      "import math\n",
      "import numpy\n",
      "\n",
      "class Options:\n",
      "    def __init__(self, threshold_m, threshold_n, max_iteration, tolerance, num_pc):\n",
      "        self.threshold_m = threshold_m\n",
      "        self.threshold_n = threshold_n\n",
      "        self.max_iteration = max_iteration\n",
      "        self.tolerance = tolerance\n",
      "        self.num_pc = num_pc\n",
      "\n",
      "class Iterator:\n",
      "    def __init__(self, options, M):\n",
      "        self.M = scipy.sparse.csc_matrix(M)\n",
      "        self.options = options\n",
      "    def thresh(self, s_vector, num_entries):\n",
      "        listOfValues = s_vector.T.todense().tolist()[0]\n",
      "        indices = self.sort(listOfValues)[1]\n",
      "        thresholded_vec = numpy.zeros(shape=s_vector.T.shape)\n",
      "        for i in range(num_entries):\n",
      "            index = indices[i]\n",
      "            thresholded_vec[0, index] = listOfValues[index]\n",
      "        return scipy.sparse.csr_matrix(thresholded_vec).T\n",
      "\n",
      "    def top_k_ind(self, s_vector, num_entries):\n",
      "        listOfValues = s_vector.T.todense().tolist()[0]\n",
      "        return self.sort(listOfValues)\n",
      "\n",
      "    def sort(self, vector):\n",
      "        indices = range(len(vector))\n",
      "        val_ind = map (lambda x : (abs(vector[x]),x), indices)\n",
      "        sorted_val_ind = sorted(val_ind)\n",
      "        sorted_val_ind.reverse()\n",
      "        sorted_indices = map (lambda x : x[1], sorted_val_ind)\n",
      "        sorted_values = map (lambda x : x[0], sorted_val_ind)\n",
      "        return (sorted_values, sorted_indices)\n",
      "    def single_iteration (self):\n",
      "\n",
      "        M = self.M\n",
      "        m = self.M.shape[0]\n",
      "        n = self.M.shape[1]\n",
      "        k_p = self.options.threshold_m\n",
      "        k_q = self.options.threshold_n\n",
      "        tolerance = self.options.tolerance\n",
      "        max_iteration = self.options.max_iteration\n",
      "\n",
      "\n",
      "        ini = self.ini_mat();\n",
      "        p = ini[0]\n",
      "        q = ini[1]\n",
      "        obj0 = float(\"inf\")\n",
      "        converged = False\n",
      "        iter = 0\n",
      "        while not converged:\n",
      "            p_new = self.thresh((M*q), k_p)\n",
      "            #print 'value is ' + str(sum(map(lambda x: x*x, p_new.T.todense().tolist()[0])))\n",
      "            #print 'max p is' + str(max(p_new.todense().tolist()[0]))\n",
      "            p = p_new\n",
      "            #print 'value is ' + str(p_new.todense())\n",
      "            #print 'q thresholding: '\n",
      "            q_new = self.thresh((p.T * M).T, k_q)\n",
      "            q = q_new/sum(map(abs,(q_new.data)))\n",
      "            q_test = (p.T * M)\n",
      "            #print 'max qt is' + str((p.T * M).T.shape)\n",
      "            #print 'max q is' + str(max(q_new.todense().tolist()[0]))\n",
      "            updated_mat = M-p_new*(q_new.T)\n",
      "            obj1 = updated_mat.data.dot(updated_mat.data)\n",
      "            if (abs(obj1-obj0) <= tolerance or iter >= max_iteration):\n",
      "                converged = 1\n",
      "            iter = iter + 1\n",
      "            obj0 = obj1\n",
      "        return (p,q)\n",
      "\n",
      "    def multiple_iterations (self):\n",
      "        term_inds = []\n",
      "        doc_inds = []\n",
      "        term_vals = []\n",
      "        doc_vals = []\n",
      "        for i in range(self.options.num_pc):\n",
      "            print 'handling pc# ' + str(i)\n",
      "            (p,q) = self.single_iteration()\n",
      "            (p_val, p_ind) = self.top_k_ind(p, self.options.threshold_m)\n",
      "            (q_val, q_ind) = self.top_k_ind(q, self.options.threshold_n)\n",
      "            doc_inds.append(p_ind[0:self.options.threshold_m])\n",
      "            term_inds.append(q_ind[0:self.options.threshold_n])\n",
      "            doc_vals.append(p_val[0:self.options.threshold_m])\n",
      "            term_vals.append(q_val[0:self.options.threshold_n])\n",
      "            self.remove_cols(q_ind[0:self.options.threshold_n])\n",
      "            self.remove_rows(p_ind[0:self.options.threshold_m])\n",
      "\n",
      "        return ((doc_vals,doc_inds),(term_vals,term_inds))\n",
      "\n",
      "    @staticmethod\n",
      "    def run(matrix, dict, titlelist, categorylist, output_file_name):\n",
      "        print 'running'\n",
      "\n",
      "        opt = Options(5, 5, 50, 0.0001, 3)\n",
      "        it = Iterator(opt, matrix)\n",
      "        word_pcs = []\n",
      "        title_pcs = []\n",
      "\n",
      "        dwords='var dwords=['\n",
      "        fnames = 'var fnames=['\n",
      "        links = 'var links=['\n",
      "        values = 'var values=['\n",
      "        fvalues = 'var fvalues=['\n",
      "\n",
      "        ((p_vals,p_inds),(q_vals,q_inds)) = it.multiple_iterations()\n",
      "        for pc, pcv in zip(q_inds, q_vals):\n",
      "            word_pc = []\n",
      "            dwordstr = '['\n",
      "            valstr = '['\n",
      "            for q_ind, q_val in zip(pc, pcv):\n",
      "                word_pc.append('%.4f\\t%s' % (q_val, dict[q_ind]))\n",
      "                dwordstr = dwordstr + format('\"%s\",' %(dict[q_ind]))\n",
      "                valstr = valstr + format('%.3f,' %(q_val))\n",
      "            dwordstr = dwordstr[0:len(dwordstr)-1] + ']'\n",
      "            valstr = valstr[0:len(valstr)-1] + ']'\n",
      "            dwords = dwords+dwordstr+','\n",
      "            values = values + valstr+','\n",
      "\n",
      "            word_pcs.append(word_pc)\n",
      "            todel = sorted(pc);\n",
      "            todel.reverse();\n",
      "            for q_ind in todel:\n",
      "                del dict[q_ind]\n",
      "        for pc, pcv in zip(p_inds, p_vals):\n",
      "            title_pc = []\n",
      "            fnamestr = '['\n",
      "            fvalstr = '['\n",
      "            linkstr = '['\n",
      "            for p_ind,p_val in zip(pc,pcv):\n",
      "                title_pc.append('%.4f\\t%s\\t%s' % (p_val,titlelist[p_ind],categorylist[p_ind]))\n",
      "                fnamestr = fnamestr + format('\"%s\",' %(titlelist[p_ind]))\n",
      "                linkstr = linkstr + format('\"%s\",' %(categorylist[p_ind]))\n",
      "                fvalstr = fvalstr + format('%.3f,' %(p_val))\n",
      "            fnamestr = fnamestr[0:len(fnamestr)-1] + ']'\n",
      "            fvalstr = fvalstr[0:len(fvalstr)-1] + ']'\n",
      "            linkstr = linkstr[0:len(linkstr)-1] + ']'\n",
      "\n",
      "            fnames = fnames + fnamestr+','\n",
      "            fvalues = fvalues + fvalstr + ','\n",
      "            links = links + linkstr + ',';\n",
      "            title_pcs.append(title_pc)\n",
      "            todel = sorted(pc);\n",
      "            todel.reverse();\n",
      "            for p_ind in todel:\n",
      "                del titlelist[p_ind]\n",
      "                del categorylist[p_ind]\n",
      "        dwords=dwords[0:len(dwords)-1]+'];';\n",
      "        fnames = fnames[0:len(fnames)-1]+'];';\n",
      "        links = links[0:len(links)-1]+'];';\n",
      "        values = values[0:len(values)-1]+'];';\n",
      "        fvalues = fvalues[0:len(fvalues)-1]+'];';\n",
      "        outputfile = open(output_file_name,'w')\n",
      "        outputfile.write('%s\\n%s\\n%s\\n%s\\n%s\\n'%(dwords, fnames, links, values, fvalues))\n",
      "        print 'Final results:'\n",
      "        print (dwords, fnames, links, values, fvalues)\n",
      "\n",
      "    def remove_rows(self, inds_to_remove):\n",
      "        self.M = self.M.T\n",
      "        self.remove_cols(inds_to_remove)\n",
      "        self.M = self.M.T\n",
      "    def remove_cols(self, inds_to_remove):\n",
      "        inds_to_remove = sorted(inds_to_remove)\n",
      "        inds_to_remove.reverse()\n",
      "        for ind in inds_to_remove:\n",
      "            self.remove_col(ind)\n",
      "\n",
      "    def remove_col(self, ind):\n",
      "        if ind == 0:\n",
      "            self.M = self.M[:,ind+1:].tocsc()\n",
      "        elif ind == (self.M.shape[1]-1):\n",
      "            self.M = self.M[:,0:ind].tocsc()\n",
      "        else:\n",
      "            self.M = scipy.sparse.hstack([self.M[:,0:ind],self.M[:,ind+1:]]).tocsc()\n",
      "\n",
      "    def ini_mat(self):\n",
      "        p = scipy.sparse.csc_matrix(numpy.ones(shape=(self.M.shape[0],1)))\n",
      "        q = scipy.sparse.csc_matrix(numpy.ones(shape=(self.M.shape[1],1)))\n",
      "        p = p/math.sqrt(p.T.dot(p).data[0])\n",
      "        q = q/math.sqrt(q.T.dot(q).data[0])\n",
      "        return (p,q)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# topic_summarizer.py\n",
      "import urllib2\n",
      "import nltk\n",
      "import csv\n",
      "import re\n",
      "import numpy as np\n",
      "import scipy\n",
      "from cookielib import CookieJar\n",
      "from bs4 import BeautifulSoup\n",
      "'''\n",
      "INPUT PARAMETERS\n",
      "'''\n",
      "cj = CookieJar()\n",
      "uri = \"http://export.arxiv.org/rss/cs\"\n",
      "\n",
      "class RSS_item():\n",
      "  def __init__(self, eltdict, all_docs_words):\n",
      "    self.title = eltdict['title']\n",
      "    self.source= eltdict['source']\n",
      "    self.content=eltdict['content'].lower().replace('\\n',' ')\n",
      "    self.content = re.sub(r'[^a-z]', ' ', self.content)\n",
      "    self.all_docs_words = all_docs_words\n",
      "    #print self.content\n",
      "    self.all_words={}\n",
      "\n",
      "    stopWords = set()\n",
      "    cr = csv.reader(open('./stopwords.txt'))\n",
      "    words=[]\n",
      "    for word in cr:\n",
      "     stopWords.add(word[0])\n",
      "\n",
      "    for word in nltk.word_tokenize(self.content):\n",
      "      if word not in stopWords:\n",
      "        self.all_docs_words.add(word)\n",
      "        if self.all_words.has_key(word):\n",
      "          self.all_words[word]=self.all_words[word] + 1\n",
      "        else:\n",
      "          self.all_words[word]=1\n",
      "\n",
      "def create_sparse_matrix_data(all_docs_words, all_docs):\n",
      "  rows, cols, entries = [], [], []\n",
      "  for document_index, doc in enumerate(all_docs):\n",
      "    for word in doc.all_words:\n",
      "      if doc.all_words[word] != 0:\n",
      "        rows.append(document_index)\n",
      "        cols.append(all_docs_words.index(word))\n",
      "        entries.append(doc.all_words[word])\n",
      "  rows, cols, entries = np.array(rows), np.array(cols), np.array(entries)\n",
      "  return scipy.sparse.csc_matrix((entries, (rows, cols)))\n",
      "\n",
      "def extract_data(uri):\n",
      "  all_docs_words = set()\n",
      "  opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n",
      "  html = opener.open(uri).read()\n",
      "  dicts = [];\n",
      "  all_docs = []\n",
      "  soup = BeautifulSoup(html,'xml')\n",
      "  for elt in soup.findAll('item'):\n",
      "    titlestr = elt.find('title').getText()\n",
      "    linkstr = elt.find('link').getText()\n",
      "    descstr = elt.find('description').getText()\n",
      "    eltdict = {};\n",
      "    eltdict['title']=titlestr.encode('utf-8')\n",
      "    eltdict['content']=descstr.encode('utf-8')\n",
      "    eltdict['source']=linkstr.encode('utf-8')\n",
      "    dicts.append(eltdict)\n",
      "\n",
      "  for doc in dicts:\n",
      "    all_docs.append(RSS_item(doc, all_docs_words))\n",
      "  all_docs_words=list(all_docs_words)\n",
      "\n",
      "  # Get links\n",
      "  link_strings = [doc['source'] for doc in dicts]\n",
      "\n",
      "  # Get document titles\n",
      "  doc_titles = [doc['title'] for doc in dicts]\n",
      "\n",
      "  # Create wordlist\n",
      "  wordlist = [word for word in all_docs_words]\n",
      "\n",
      "  # Create the sparse matrix data\n",
      "  sparse_matrix_data = create_sparse_matrix_data(all_docs_words, all_docs)\n",
      "  return sparse_matrix_data, wordlist, doc_titles, link_strings\n",
      "\n",
      "def summarize_text():\n",
      "  sparse_matrix_data, wordlist, doc_titles, link_strings = extract_data(uri)\n",
      "  Iterator.run(sparse_matrix_data, wordlist,\n",
      "               doc_titles, link_strings, 'output.js')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summarize_text()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "running\n",
        "handling pc# 0\n",
        "handling pc# 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "handling pc# 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Final results:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('var dwords=[[\"precoding\",\"channel\",\"constellation\",\"ce\",\"receiver\"],[\"data\",\"electrification\",\"mobile\",\"phone\",\"energy\"],[\"separation\",\"cluster\",\"means\",\"recovery\",\"clusters\"]];', 'var fnames=[[\"Constant Envelope Precoding with Adaptive Receiver Constellation in Fading Channel. (arXiv:1503.09178v3 [cs.IT] UPDATED)\",\"A Unified Scheme for Two-Receiver Broadcast Channels with Receiver Message Side Information. (arXiv:1504.00082v2 [cs.IT] UPDATED)\",\"Wireless Compressive Sensing Over Fading Channels with Distributed Sparse Random Projections. (arXiv:1504.03974v1 [cs.IT])\",\"On Reachability for Unidirectional Channel Systems Extended with Regular Tests. (arXiv:1406.5067v4 [cs.LO] UPDATED)\",\"The Effect of Maximal Rate Codes on the Interfering Message Rate. (arXiv:1404.6690v3 [cs.IT] UPDATED)\"],[\"Using Mobile Phone Data for Electricity Infrastructure Planning. (arXiv:1504.03899v1 [physics.soc-ph])\",\"Mutiscale Mapper: A Framework for Topological Summarization of Data and Maps. (arXiv:1504.03763v1 [cs.CG])\",\"Design Issues of JPQ: a Pattern-based Query Language for Document Databases. (arXiv:1504.03770v1 [cs.DB])\",\"Variational Recurrent Auto-Encoders. (arXiv:1412.6581v5 [stat.ML] UPDATED)\",\"SPIKY: A graphical user interface for monitoring spike train synchrony. (arXiv:1410.6910v3 [physics.data-an] UPDATED)\"],[\"Relax, no need to round: integrality of clustering formulations. (arXiv:1408.4045v5 [stat.ML] UPDATED)\",\"Probabilistic Clustering of Time-Evolving Distance Data. (arXiv:1504.03701v1 [cs.LG])\",\"Stability in human interaction networks: primitive typology of vertex, prominence of measures and activity statistics. (arXiv:1310.7769v5 [cs.SI] UPDATED)\",\"Random Laplacian matrices and convex relaxations. (arXiv:1504.03987v1 [math.PR])\",\"Byzantine Agreement with Optimal Early Stopping, Optimal Resilience and Polynomial Complexity. (arXiv:1504.02547v2 [cs.DC] UPDATED)\"]];', 'var links=[[\"http://arxiv.org/abs/1503.09178\",\"http://arxiv.org/abs/1504.00082\",\"http://arxiv.org/abs/1504.03974\",\"http://arxiv.org/abs/1406.5067\",\"http://arxiv.org/abs/1404.6690\"],[\"http://arxiv.org/abs/1504.03899\",\"http://arxiv.org/abs/1504.03763\",\"http://arxiv.org/abs/1504.03770\",\"http://arxiv.org/abs/1412.6581\",\"http://arxiv.org/abs/1410.6910\"],[\"http://arxiv.org/abs/1408.4045\",\"http://arxiv.org/abs/1504.03701\",\"http://arxiv.org/abs/1310.7769\",\"http://arxiv.org/abs/1504.03987\",\"http://arxiv.org/abs/1504.02547\"]];', 'var values=[[0.223,0.210,0.195,0.195,0.176],[0.373,0.188,0.157,0.157,0.125],[0.234,0.206,0.196,0.196,0.167]];', 'var fvalues=[[6.661,1.512,0.842,0.597,0.597],[5.436,2.239,1.865,1.492,1.119],[5.066,0.747,0.196,0.196,0.000]];')\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}