{
 "metadata": {
  "name": "",
  "signature": "sha256:a24d7e98e303941f23b80060053f48ab12c5ff2614ad1b9fcfb5daef9f253852"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy\n",
      "import numpy as np\n",
      "from scipy.sparse import csr_matrix\n",
      "\n",
      "# Load bag of words data\n",
      "nytimes_file = open(\"docword.nytimes.txt\")\n",
      "n_documents = int(nytimes_file.readline())\n",
      "n_words_distinct = int(nytimes_file.readline())\n",
      "n_words_total = int(nytimes_file.readline())\n",
      "print(\"Number of docs: %d, vocabulary size: %d, total words: %d\" % (n_documents, n_words_distinct, n_words_total))\n",
      "\n",
      "# Construct intermediary sparse data\n",
      "datas, row_inds, col_inds = [], [], []\n",
      "for line in nytimes_file:\n",
      "    if line.strip() == \"\":\n",
      "        continue\n",
      "    row, col, count = (float(x) for x in line.strip().split())\n",
      "    datas.append(count)\n",
      "    row_inds.append(row-1)\n",
      "    col_inds.append(col-1)\n",
      "nytimes_file.close()\n",
      "    \n",
      "# Get the vocabulary\n",
      "vocab_file = open(\"vocab.nytimes.txt\")\n",
      "vocabulary, word_to_id_map = [], {}\n",
      "for i, line in enumerate(vocab_file):\n",
      "    current_word = line.strip()\n",
      "    vocabulary.append(current_word)\n",
      "    word_to_id_map[current_word] = i\n",
      "vocab_file.close()\n",
      "    \n",
      "# Create the sparse csr matrix\n",
      "print(\"Creating sparse matrix...\")\n",
      "sparse_matrix = csr_matrix((datas, (row_inds, col_inds)))\n",
      "print(\"Matrix dimensions\", sparse_matrix.shape)\n",
      "print(\"DONE\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of docs: 300000, vocabulary size: 102660, total words: 69679427\n",
        "Creating sparse matrix..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Matrix dimensions', (300000, 102660))"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "DONE\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import izip\n",
      "\n",
      "query_term = \"sex\"\n",
      "\n",
      "# Create the classification vector\n",
      "index = word_to_id_map[query_term]\n",
      "cx = scipy.sparse.coo_matrix(sparse_matrix)\n",
      "classification_vector = [-1 for x in range(n_documents)]\n",
      "for row, col in izip(cx.row, cx.col):\n",
      "    if col-1 == index:\n",
      "        classification_vector[row-1] = 1\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Copyright (c) 2014 Steve Yadlowsky, Preetum Nakkarin.\n",
      "# Licensed under MIT License.\n",
      "# More information including the exact terms of the License\n",
      "# can be found in the file COPYING in the project root directory.\n",
      "\n",
      "import numpy as np\n",
      "import time\n",
      "import operator\n",
      "import scipy.sparse\n",
      "import scipy.sparse.linalg\n",
      "\n",
      "class IHTClassifier(object):\n",
      "\n",
      "    def __init__(self):\n",
      "        self.training_time = 0.0\n",
      "        self.beta = None\n",
      "\n",
      "    def card(self, x):\n",
      "        return np.sum(x != 0)\n",
      "\n",
      "    def train(self, X, y, card=100, verbose=False):\n",
      "        start = time.time()\n",
      "        if verbose:\n",
      "            print \"Preconditioning matrix\"\n",
      "        whitened_X, feature_avg = self.whiten_features(X)\n",
      "        lsv = float(self.compute_lsv(whitened_X, feature_avg))\n",
      "        if verbose:\n",
      "            print \"Matching pursuits\"\n",
      "        x_hat = self.matching_pursuit_sparse(y, whitened_X/lsv, feature_avg/lsv, card)\n",
      "        if verbose:\n",
      "            print \"Running iterative hard thresholding\"\n",
      "        self.beta = self.AIHT_sparse(y, whitened_X/lsv, x_hat, card, feature_avg/lsv)/lsv\n",
      "\n",
      "        self.training_time += time.time() - start\n",
      "\n",
      "    def whiten_features(self, X):\n",
      "        X = X.tocsr(copy=True)\n",
      "        row_avg = np.bincount(X.indices, weights=X.data)\n",
      "        row_avg /= float(X.shape[0])\n",
      "        row_norm = np.bincount(X.indices, weights=(X.data - row_avg[X.indices])**2)\n",
      "        nonzeros_in_each_column = np.diff(X.tocsc().indptr)\n",
      "        avg_norm = ((float(X.shape[0])*np.ones(X.shape[1])) - nonzeros_in_each_column)*(row_avg**2)\n",
      "        row_norm += avg_norm\n",
      "        row_norm = np.array([np.sqrt(x) if x != 0 else 1 for x in row_norm])\n",
      "        row_avg /= row_norm\n",
      "        X.data /= np.take(row_norm, X.indices)\n",
      "        feature_avg = np.squeeze(row_avg)\n",
      "\n",
      "        return X, feature_avg\n",
      "\n",
      "    def compute_lsv(self, X, feature_avg):\n",
      "        def matmuldyad(v):\n",
      "            return X.dot(v) - feature_avg.dot(v)\n",
      "\n",
      "        def rmatmuldyad(v):\n",
      "            return X.T.dot(v) - v.sum()*feature_avg\n",
      "        normalized_lin_op = scipy.sparse.linalg.LinearOperator(X.shape, matmuldyad, rmatmuldyad)\n",
      "\n",
      "        def matvec_XH_X(v):\n",
      "            return normalized_lin_op.rmatvec(normalized_lin_op.matvec(v))\n",
      "\n",
      "        which='LM'\n",
      "        v0=None\n",
      "        maxiter=None\n",
      "        return_singular_vectors=False\n",
      "\n",
      "        XH_X = scipy.sparse.linalg.LinearOperator(matvec=matvec_XH_X, dtype=X.dtype, shape=(X.shape[1], X.shape[1]))\n",
      "        eigvals = scipy.sparse.linalg.eigs(XH_X, k=1, tol=0, maxiter=None, ncv=10, which=which, v0=v0, return_eigenvectors=False)\n",
      "        lsv = np.sqrt(eigvals)\n",
      "        return lsv[0].real\n",
      "\n",
      "    def matching_pursuit_sparse(self, y, X, feature_avg, k, tol=10**-10):\n",
      "        '''\n",
      "        Matching Pursuit\n",
      "        '''\n",
      "        r = y\n",
      "        X = X.tocsc()\n",
      "        err_norm = np.linalg.norm(r, 2)\n",
      "        err_norm_prev = 0\n",
      "        beta = np.zeros(X.shape[1])\n",
      "        while self.card(beta) < k:\n",
      "            all_inner_products = X.T.dot(r) - np.sum(r)*feature_avg\n",
      "            max_index, max_abs_inner_product = max(enumerate(np.abs(all_inner_products)), key=operator.itemgetter(1))\n",
      "            g = X[:, max_index]\n",
      "            g = np.squeeze(np.asarray(g.todense())) - feature_avg[max_index]\n",
      "            a = all_inner_products[max_index]\n",
      "            a /= np.linalg.norm(g, 2)**2\n",
      "            beta[max_index] += a\n",
      "            r = r - a*g\n",
      "            err_norm_prev = err_norm\n",
      "            err_norm = np.linalg.norm(r, 2)\n",
      "            if np.abs(err_norm - err_norm_prev) <= tol:\n",
      "                break\n",
      "        return beta\n",
      "\n",
      "    def thresholder(self, y,m):\n",
      "        sort_y = sorted(np.abs(y))\n",
      "        thresh = sort_y[-m]\n",
      "\n",
      "        non_thresholded_indices = (np.abs(y) > thresh)\n",
      "        n_nonzero_indices = sum(non_thresholded_indices)\n",
      "        if n_nonzero_indices < m:\n",
      "            collisions = np.where((np.abs(y)==thresh))[0]\n",
      "            passed = np.random.choice(collisions,m-n_nonzero_indices)\n",
      "            non_thresholded_indices[passed] = 1\n",
      "\n",
      "        y_new = non_thresholded_indices * y\n",
      "\n",
      "        return y_new, thresh\n",
      "\n",
      "    def AIHT_sparse(self, y, X, beta, k, feature_avg=None, alpha=0, example_weights=None, max_iters=10000, tol=10**-16):\n",
      "        \"\"\"Solves DORE accelerated IHT with a sparse matrix X.\n",
      "        \"\"\"\n",
      "        m, n = X.shape\n",
      "        y = np.squeeze(np.asarray(y))\n",
      "        err_norm_prev = 0\n",
      "        beta_0 = beta\n",
      "        beta_prev = beta\n",
      "        X_beta = 0\n",
      "        X_beta_prev = 0\n",
      "        X_beta_twice_prev = 0\n",
      "\n",
      "        if feature_avg is None:\n",
      "            feature_avg = np.zeros(n)\n",
      "\n",
      "        if example_weights is None:\n",
      "            example_weights = np.ones(m)\n",
      "\n",
      "        for iter_ in xrange(max_iters):\n",
      "            X_beta_twice_prev = X_beta_prev\n",
      "            X_beta_prev = X_beta\n",
      "            X_beta = (X.dot(beta) - feature_avg.dot(beta))\n",
      "            X_beta = np.squeeze(np.asarray(X_beta))\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta\n",
      "            norm_change = ((np.linalg.norm(beta - beta_prev)**2)/n)\n",
      "            print err.dot(err) + err_reg.dot(err_reg), norm_change, np.linalg.norm(beta)\n",
      "\n",
      "            if iter_ > 0 and (norm_change <= tol):\n",
      "                break\n",
      "\n",
      "            beta_t = beta + np.squeeze(np.asarray(X.T.dot(err))) - err.sum()*feature_avg + alpha*err_reg\n",
      "            beta_t = np.squeeze(np.asarray(beta_t))\n",
      "\n",
      "            beta_t, thresh = self.thresholder(beta_t,k)\n",
      "            X_beta = X.dot(beta_t) - feature_avg.dot(beta_t)\n",
      "            X_beta = np.squeeze(X_beta)\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta_t\n",
      "\n",
      "            beta_t_star = beta_t\n",
      "            if iter_ > 2:\n",
      "                delta_X_beta = X_beta - X_beta_prev\n",
      "                delta_regularization = alpha*(beta_t - beta)\n",
      "                dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                if dp > 0:\n",
      "                    a1 = (delta_X_beta.dot(err) + delta_regularization.dot(err_reg))/dp\n",
      "                    X_beta_1 = (1+a1)*X_beta - a1*X_beta_prev\n",
      "                    beta_1 = beta_t + a1*(beta_t - beta)\n",
      "                    err_1 = y - example_weights*X_beta_1\n",
      "                    err_1_reg = -alpha*beta_1\n",
      "\n",
      "                    delta_X_beta = X_beta_1 - X_beta_twice_prev\n",
      "                    delta_regularization = alpha*(beta_1 - beta_prev)\n",
      "                    dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                    if dp > 0:\n",
      "                        a2 = (delta_X_beta.dot(err_1) + delta_regularization.dot(err_1_reg))/dp\n",
      "                        beta_2 = beta_1 + a2*(beta_1 - beta_prev)\n",
      "                        beta_2, thresh = self.thresholder(beta_2,k)\n",
      "\n",
      "                        X_beta_2 = X.dot(beta_2) - feature_avg.dot(beta_2)\n",
      "                        X_beta_2 = np.squeeze(np.asarray(X_beta_2))\n",
      "                        err_2 = y - example_weights*X_beta_2\n",
      "                        err_reg_2 = -alpha*beta_2\n",
      "\n",
      "                        if (err_2.dot(err_2) + err_reg_2.dot(err_reg_2)) / (err.dot(err) + err_reg.dot(err_reg)) < 1:\n",
      "                            beta_t_star = beta_2\n",
      "                            X_beta = X_beta_2\n",
      "\n",
      "            beta_prev = beta\n",
      "            beta = beta_t_star\n",
      "\n",
      "        return beta\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Classification vector length\", len(classification_vector))\n",
      "print(\"Matrix shape\", sparse_matrix.shape)\n",
      "\n",
      "classifier = IHTClassifier()\n",
      "classifier.train(sparse_matrix, classification_vector)\n",
      "weights = classifier.beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Classification vector length', 300000)\n",
        "('Matrix shape', (300000, 102660))\n",
        "299978.203592"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0 75.6177550629\n",
        "299978.200783"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.37087874298e-08 75.6305006839\n",
        "299978.197996"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.36015739752e-08 75.643256381\n",
        "299978.19523"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.34957181354e-08 75.6560219712\n",
        "299977.83959"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.000896174827487 79.4706999437\n",
        "299977.557621"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0014528922225 86.9033763929\n",
        "299977.42377"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.000687187492391 93.4581765269\n",
        "299977.318608"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.000808376940412 98.5055022882\n",
        "299977.244314"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.00064761156867 101.71094146\n",
        "299977.227874"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7.28219617523e-05 102.570984668\n",
        "299977.220718"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.28477816413e-05 103.27420088\n",
        "299977.218863"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.17183239054e-06 103.450052435\n",
        "299977.218561"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 7.68776985455e-07 103.450943395\n",
        "299977.218492"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.94153232903e-07 103.464239076\n",
        "299977.218476"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.16499485018e-08 103.472340558\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.33969164731e-09 103.472571024\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.02442483585e-09 103.472783753\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.16938204268e-10 103.47367115\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.08896917278e-16 103.473670943\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.02959070854e-16 103.473670738\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.52036774974e-11 103.473628976\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.25639848348e-16 103.473629123\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.80351753085e-12 103.473663182\n",
        "299977.218473"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6.53327280249e-17 103.473663188\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(sorted_terms[:100])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['zzz_mario_ruiz_massieu', 'zzz_maria_bernal', 'zzz_jacqueline_du_pre', 'zzz_adele', 'zzz_susan_linn', 'zzz_mena_suvari', 'zzz_fishing', 'zzz_hayden', 'zzz_bangles', 'zzz_jeremy_scott', 'zzz_brenda', 'l995', 'zzz_russian_navy', 'zzz_texas_medical_branch', 'zzz_leah', 'zzz_lauren_ambrose', 'zzz_hyperbole', 'zzz_trinidadian', 'ironwork', 'zzz_pierre_trudeau', 'zzz_keith_o_brien', 'zzz_robert_zemeckis', 'zzz_otto_schily', 'zzz_elizabeth_streb', 'zzz_fitzgerald', 'ecologist', 'wacked', 'lucidly', 'atelier', 'hopscotching', 'polymath', 'zzz_pro_am', 'copulation', 'def', 'zzz_braveheart', 'zzz_wolfgang_schuessel', 'zzz_hayden_planetarium', 'misjudged', 'zzz_austrian', 'uscourt', 'zzz_india_pakistan', 'zzz_saving_private_ryan', 'zzz_snowbird', 'mfeeney', 'wades', 'zzz_ziegler', 'zzz_tommy_flanagan', 'fearfulness', 'zzz_pink', 'suavity', 'statuary', 'zzz_hepa', 'zzz_patterson', 'zzz_puffy', 'zzz_royal_family', 'zzz_verisign_inc', 'ganging', 'zzz_bamiyan', 'shilling', 'visionaries', 'overanalyze', 'zzz_griffith', 'nondiscriminatory', 'zzz_systems_inc', 'zzz_calypso', 'sexed', 'zzz_john_cogan', 'desensitization', 'yammering', 'haughtiness', 'zzz_frances', 'zzz_muriel', 'zzz_click', 'conversant', 'zzz_frederick_cohn', 'unprepossessing', 'stagecraft', 'disharmony', 'zzz_maya', 'zzz_al_hayat', 'ole', 'zzz_oceania', 'patriotically', 'aah', 'aahed', 'aaron', 'aback', 'abacus', 'abajo', 'abalone', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abandono', 'abarnard', 'abashed', 'abate', 'abated', 'abatement']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}