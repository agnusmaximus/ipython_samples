{
 "metadata": {
  "name": "",
  "signature": "sha256:ee0ae1592fe83c623b189da530b55d370003af575495bd9170ba6ef2d266fbe6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################################################\n",
      "# Creates a term document matrix from movie reviews data          #\n",
      "###################################################################\n",
      "import sys\n",
      "import os\n",
      "import glob\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "START_K = 0\n",
      "K = 800\n",
      "\n",
      "# Movie data corpus\n",
      "negative_files = glob.glob(\"movie_review_data/neg/*.txt\")\n",
      "positive_files = glob.glob(\"movie_review_data/pos/*.txt\")\n",
      "all_file_names = negative_files + positive_files\n",
      "\n",
      "# Load valid english words\n",
      "valid_words = set()\n",
      "dictionary_file = open(\"words\")\n",
      "for word in dictionary_file:\n",
      "    word = word.lower().strip()\n",
      "    valid_words.add(word)\n",
      "    \n",
      "# Load english stop words\n",
      "stop_words = set()\n",
      "stop_words_file = open(\"stop_words\")\n",
      "for word in stop_words_file:\n",
      "    word = word.lower().strip()\n",
      "    stop_words.add(word)\n",
      "\n",
      "# Filter even more. Only take the K top most frequent words from the combined corpus\n",
      "word_frequencies = {}\n",
      "corpus_words = set()\n",
      "for file_name in all_file_names:\n",
      "    f = open(file_name)\n",
      "    text = unicode(f.read(), 'latin-1')\n",
      "    for word in text.split(\" \"):\n",
      "        word = word.lower().strip()\n",
      "        if word in valid_words and word not in stop_words:\n",
      "            if word not in word_frequencies:\n",
      "                word_frequencies[word] = 0\n",
      "            word_frequencies[word] += 1\n",
      "            corpus_words.add(word)\n",
      "    f.close()\n",
      "valid_features = sorted(corpus_words, key=lambda x: word_frequencies[x], reverse=True)[START_K:START_K+K]\n",
      "                \n",
      "# Collect all movie review text (both positive and negatively sentimented)\n",
      "corpus = []\n",
      "for count, file_name in enumerate(all_file_names):\n",
      "    f = open(file_name)\n",
      "    document_string = unicode(f.read(), 'latin-1')\n",
      "    filtered_document_string = \"\"\n",
      "    for word in document_string.split(\" \"):\n",
      "        word = word.lower().strip()\n",
      "        if word in valid_features:\n",
      "            filtered_document_string += word + ' '\n",
      "    corpus.append(filtered_document_string)\n",
      "    f.close()\n",
      "print(\"Done creating corpus!\")\n",
      "\n",
      "# Use Sklearn to get the term document matrix\n",
      "vectorizer = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
      "term_doc_matrix = vectorizer.fit_transform(corpus)\n",
      "\n",
      "# tdm and features\n",
      "tdm = term_doc_matrix.toarray()\n",
      "features = vectorizer.get_feature_names()\n",
      "\n",
      "print(\"Number of rows: %d cols: %d\" % tdm.shape)\n",
      "print(tdm)\n",
      "print(\"Features:\")\n",
      "print(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done creating corpus!\n",
        "Number of rows: 1400 cols: 758"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.06564099  0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " ..., \n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.04419619  0.         ...,  0.          0.          0.        ]]\n",
        "Features:\n",
        "[u'ability', u'able', u'absolutely', u'accident', u'act', u'acting', u'action', u'actor', u'actress', u'actual', u'actually', u'adam', u'add', u'admit', u'affair', u'age', u'agent', u'ago', u'air', u'alien', u'allen', u'amazing', u'america', u'american', u'amusing', u'anderson', u'animated', u'animation', u'annoying', u'answer', u'apart', u'apartment', u'apparently', u'appear', u'appearance', u'approach', u'army', u'art', u'aside', u'aspect', u'atmosphere', u'attempt', u'attention', u'audience', u'awful', u'baby', u'background', u'bad', u'baldwin', u'band', u'barely', u'barry', u'based', u'basically', u'batman', u'battle', u'beautiful', u'beauty', u'begin', u'beginning', u'believable', u'believe', u'ben', u'biggest', u'billy', u'bit', u'bizarre', u'black', u'blood', u'blue', u'bob', u'body', u'bond', u'book', u'boring', u'boss', u'boy', u'break', u'brian', u'brief', u'brilliant', u'bring', u'british', u'brother', u'brought', u'brown', u'bruce', u'budget', u'building', u'bunch', u'business', u'cage', u'camera', u'car', u'care', u'career', u'carry', u'cast', u'casting', u'catch', u'caught', u'central', u'chance', u'change', u'character', u'charles', u'charlie', u'charm', u'chase', u'cheap', u'check', u'chemistry', u'child', u'choice', u'chris', u'christopher', u'cinema', u'cinematic', u'cinematography', u'city', u'class', u'classic', u'clever', u'climax', u'close', u'club', u'cold', u'college', u'comedy', u'comes', u'comic', u'coming', u'company', u'complete', u'completely', u'complex', u'computer', u'conclusion', u'consider', u'considering', u'constantly', u'contact', u'control', u'convincing', u'cool', u'cop', u'copyright', u'country', u'couple', u'course', u'create', u'credit', u'crew', u'crime', u'critic', u'culture', u'cut', u'cute', u'danny', u'dark', u'date', u'daughter', u'david', u'day', u'days', u'dead', u'deal', u'death', u'debut', u'decent', u'deep', u'definitely', u'dennis', u'depth', u'desperate', u'despite', u'detective', u'development', u'dialogue', u'die', u'difficult', u'directed', u'direction', u'director', u'disaster', u'doctor', u'dog', u'doing', u'don', u'door', u'doubt', u'drama', u'dramatic', u'dream', u'drug', u'dull', u'dumb', u'earth', u'easily', u'easy', u'eddie', u'edward', u'effect', u'effective', u'effects', u'effort', u'element', u'emotional', u'energy', u'english', u'enjoy', u'enjoyable', u'entertaining', u'entertainment', u'entire', u'entirely', u'escape', u'especially', u'event', u'eventually', u'evil', u'exactly', u'example', u'excellent', u'exciting', u'expect', u'experience', u'extremely', u'eye', u'fairly', u'fall', u'familiar', u'family', u'famous', u'fan', u'fast', u'father', u'favorite', u'fear', u'feature', u'feel', u'feeling', u'fellow', u'female', u'fiction', u'fight', u'figure', u'filled', u'film', u'final', u'finale', u'finally', u'fine', u'flat', u'flick', u'focus', u'follow', u'following', u'force', u'forced', u'forget', u'form', u'fox', u'frank', u'free', u'french', u'friend', u'fun', u'funny', u'future', u'game', u'gay', u'genre', u'genuine', u'george', u'getting', u'giant', u'girl', u'giving', u'god', u'goes', u'gone', u'gore', u'government', u'grace', u'ground', u'guess', u'gun', u'guy', u'half', u'hand', u'happen', u'happy', u'hard', u'hardly', u'harry', u'hate', u'head', u'hear', u'heart', u'hell', u'help', u'henry', u'hero', u'highly', u'hilarious', u'history', u'hit', u'hold', u'hollywood', u'home', u'hope', u'horrible', u'horror', u'hot', u'hour', u'house', u'huge', u'human', u'humor', u'husband', u'idea', u'imagine', u'immediately', u'impact', u'impossible', u'impressive', u'incredibly', u'information', u'inside', u'instead', u'intelligence', u'intelligent', u'involved', u'island', u'jack', u'james', u'jamie', u'jason', u'jay', u'jeff', u'jennifer', u'jerry', u'jim', u'job', u'joe', u'john', u'joke', u'jonathan', u'jones', u'julia', u'kevin', u'key', u'kids', u'kill', u'killer', u'killing', u'king', u'la', u'lack', u'lady', u'language', u'late', u'laugh', u'law', u'lead', u'leader', u'leading', u'leads', u'learn', u'leave', u'leaves', u'leaving', u'led', u'lee', u'left', u'level', u'life', u'light', u'line', u'list', u'little', u'live', u'living', u'local', u'look', u'looking', u'lost', u'lot', u'lots', u'loud', u'love', u'low', u'main', u'major', u'male', u'mark', u'married', u'mars', u'martin', u'mary', u'master', u'material', u'matt', u'matter', u'matthew', u'max', u'maybe', u'mean', u'meet', u'memorable', u'mention', u'merely', u'mess', u'message', u'michael', u'middle', u'mike', u'million', u'mind', u'minor', u'minute', u'miss', u'mission', u'modern', u'moment', u'money', u'mother', u'motion', u'movie', u'moving', u'murder', u'murphy', u'music', u'musical', u'mysterious', u'mystery', u'nature', u'nearly', u'net', u'news', u'nice', u'nick', u'night', u'note', u'novel', u'nudity', u'numerous', u'obvious', u'obviously', u'occasionally', u'office', u'oh', u'oliver', u'original', u'oscar', u'outside', u'overall', u'pace', u'pain', u'park', u'particular', u'particularly', u'partner', u'party', u'pass', u'past', u'patrick', u'paul', u'pay', u'people', u'perfect', u'perfectly', u'performance', u'person', u'personal', u'personality', u'peter', u'picture', u'piece', u'plan', u'planet', u'play', u'plenty', u'plot', u'police', u'political', u'poor', u'popular', u'potential', u'power', u'powerful', u'predictable', u'premise', u'presence', u'president', u'pretty', u'previous', u'prison', u'private', u'probably', u'produced', u'producer', u'production', u'profanity', u'project', u'prove', u'provide', u'public', u'pull', u'purpose', u'quality', u'question', u'quick', u'quickly', u'race', u'rare', u'rated', u'rating', u'read', u'ready', u'real', u'realistic', u'reality', u'realize', u'reason', u'recent', u'recently', u'recommend', u'red', u'relationship', u'release', u'remains', u'remake', u'remember', u'rest', u'result', u'return', u'review', u'rich', u'richard', u'ride', u'ridiculous', u'road', u'robert', u'robin', u'rock', u'roger', u'role', u'romance', u'romantic', u'run', u'running', u'russell', u'sad', u'sadly', u'sam', u'save', u'saving', u'saying', u'scary', u'scene', u'school', u'science', u'score', u'scott', u'scream', u'screen', u'screenplay', u'screenwriter', u'script', u'sean', u'secret', u'seeing', u'seemingly', u'seen', u'sense', u'sent', u'sequel', u'sequence', u'series', u'seriously', u'set', u'setting', u'sex', u'sexual', u'share', u'ship', u'short', u'shot', u'shown', u'sight', u'silly', u'similar', u'simon', u'simple', u'simply', u'single', u'sister', u'sit', u'sitting', u'situation', u'slightly', u'slow', u'slowly', u'smart', u'smith', u'social', u'society', u'solid', u'somewhat', u'son', u'song', u'soon', u'sort', u'sound', u'south', u'space', u'speak', u'special', u'species', u'spend', u'spent', u'stand', u'star', u'starring', u'start', u'stay', u'stephen', u'steve', u'steven', u'stone', u'stop', u'store', u'story', u'straight', u'strange', u'street', u'strong', u'student', u'studio', u'stuff', u'stupid', u'style', u'subject', u'subplot', u'subtle', u'success', u'successful', u'suddenly', u'summer', u'supporting', u'supposed', u'surprise', u'surprisingly', u'suspense', u'sweet', u'taking', u'tale', u'talent', u'talented', u'talk', u'talking', u'tarzan', u'team', u'teen', u'teenage', u'television', u'tell', u'telling', u'tension', u'terrible', u'thanks', u'theater', u'theme', u'thinking', u'thomas', u'thriller', u'thrown', u'tim', u'time', u'times', u'titanic', u'title', u'told', u'tom', u'tone', u'tony', u'totally', u'touch', u'tough', u'town', u'track', u'trailer', u'train', u'trip', u'trouble', u'true', u'truly', u'truth', u'try', u'trying', u'twist', u'type', u'typical', u'ultimately', u'understand', u'unfortunately', u'unlike', u'usual', u'usually', u'van', u'various', u'version', u'video', u'view', u'viewer', u'villain', u'violence', u'violent', u'visit', u'visual', u'voice', u'wait', u'waiting', u'war', u'waste', u'wasted', u'watch', u'watching', u'water', u'wedding', u'west', u'white', u'wife', u'wild', u'william', u'wilson', u'win', u'wish', u'woman', u'wonder', u'wonderful', u'woody', u'word', u'world', u'worse', u'worst', u'worth', u'write', u'writer', u'writing', u'written', u'wrong', u'wrote', u'yes', u'york', u'zero']\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################################################\n",
      "# Searches query term within list of corpus                       #\n",
      "###################################################################\n",
      "\n",
      "query = unicode('batman')\n",
      "query_presence_indicators = []\n",
      "\n",
      "# Create the indicator column\n",
      "if query in features:\n",
      "    location_in_matrix = features.index(query)\n",
      "    features.remove(query)\n",
      "    \n",
      "    # Remove column with query term from tdm\n",
      "    tdm = np.delete(tdm, location_in_matrix, axis=1)\n",
      "    \n",
      "    # Construct indicator vector\n",
      "    for document in tdm:\n",
      "        if document[location_in_matrix] != 0:\n",
      "            query_presence_indicators.append(1)\n",
      "        else:\n",
      "            query_presence_indicators.append(-1)\n",
      "else:\n",
      "    print(\"Query not found\")\n",
      "\n",
      "print(\"Number of documents\", len(query_presence_indicators))\n",
      "print(\"Number of times query appears in all documents: \", sum([1 if x == 1 else 0 for x in query_presence_indicators]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Number of documents', 1400)\n",
        "('Number of times query appears in all documents: ', 95)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cvxpy import *\n",
      "import numpy as np\n",
      "import cvxopt\n",
      "from multiprocessing import Pool\n",
      "\n",
      "# Problem data.\n",
      "A = tdm\n",
      "b = query_presence_indicators\n",
      "gamma = Parameter(sign=\"positive\")\n",
      "\n",
      "# Construct the problem.\n",
      "x = Variable(len(A[0]))\n",
      "objective = Minimize(sum_squares(A*x - b) + gamma*norm(x, 1))\n",
      "p = Problem(objective)\n",
      "\n",
      "# Assign a value to gamma and find the optimal x.\n",
      "def get_x(gamma_value):\n",
      "    gamma.value = gamma_value\n",
      "    result = p.solve()\n",
      "    return x.value\n",
      "\n",
      "gammas = np.logspace(-1, 2, num=100)\n",
      "\n",
      "# Parallel computation.\n",
      "print(\"Starting computation\")\n",
      "pool = Pool(processes=1)\n",
      "x_values = pool.map(get_x, gammas)\n",
      "\n",
      "# Serial computation.\n",
      "lasso_weights = [get_x(value) for value in gammas]\n",
      "\n",
      "for v1,v2 in zip(x_values, lasso_weights):\n",
      "    if np.linalg.norm(v1 - v2) > 1e-5:\n",
      "        print \"error\"\n",
      "        \n",
      "print(\"Done!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting computation\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'x_values' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-8dc5c89093b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlasso_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlasso_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'x_values' is not defined"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Copyright (c) 2014 Steve Yadlowsky, Preetum Nakkarin.\n",
      "# Licensed under MIT License.\n",
      "# More information including the exact terms of the License\n",
      "# can be found in the file COPYING in the project root directory.\n",
      "\n",
      "import numpy as np\n",
      "import time\n",
      "import operator\n",
      "import scipy.sparse\n",
      "\n",
      "class IHTClassifier(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.training_time = 0.0\n",
      "        self.beta = None\n",
      "\n",
      "    def card(self, x):\n",
      "        return np.sum(x != 0)\n",
      "\n",
      "    def train(self, X, y, card=100, verbose=False):\n",
      "        start = time.time()\n",
      "        if verbose:\n",
      "            print \"Preconditioning matrix\"\n",
      "        whitened_X, feature_avg = self.whiten_features(X)\n",
      "        lsv = float(self.compute_lsv(whitened_X, feature_avg))\n",
      "        if verbose:\n",
      "            print \"Matching pursuits\"\n",
      "        x_hat = self.matching_pursuit_sparse(y, whitened_X/lsv, feature_avg/lsv, card)\n",
      "        if verbose:\n",
      "            print \"Running iterative hard thresholding\"\n",
      "        self.beta = self.AIHT_sparse(y, whitened_X/lsv, x_hat, card, feature_avg/lsv)/lsv\n",
      "\n",
      "        self.training_time += time.time() - start\n",
      "\n",
      "    def whiten_features(self, X):\n",
      "        X = X.tocsr(copy=True)\n",
      "        row_avg = np.bincount(X.indices, weights=X.data)\n",
      "        row_avg /= float(X.shape[0])\n",
      "        row_norm = np.bincount(X.indices, weights=(X.data - row_avg[X.indices])**2)\n",
      "        nonzeros_in_each_column = np.diff(X.tocsc().indptr)\n",
      "        avg_norm = ((float(X.shape[0])*np.ones(X.shape[1])) - nonzeros_in_each_column)*(row_avg**2)\n",
      "        row_norm += avg_norm\n",
      "        row_norm = np.array([np.sqrt(x) if x != 0 else 1 for x in row_norm])\n",
      "        row_avg /= row_norm\n",
      "        X.data /= np.take(row_norm, X.indices)\n",
      "        feature_avg = np.squeeze(row_avg)\n",
      "\n",
      "        return X, feature_avg\n",
      "\n",
      "    def compute_lsv(self, X, feature_avg):\n",
      "        def matmuldyad(v):\n",
      "            return X.dot(v) - feature_avg.dot(v)\n",
      "\n",
      "        def rmatmuldyad(v):\n",
      "            return X.T.dot(v) - v.sum()*feature_avg\n",
      "        normalized_lin_op = scipy.sparse.linalg.LinearOperator(X.shape, matmuldyad, rmatmuldyad)\n",
      "\n",
      "        def matvec_XH_X(v):\n",
      "            return normalized_lin_op.rmatvec(normalized_lin_op.matvec(v))\n",
      "\n",
      "        which='LM'\n",
      "        v0=None\n",
      "        maxiter=None\n",
      "        return_singular_vectors=False\n",
      "\n",
      "        XH_X = scipy.sparse.linalg.LinearOperator(matvec=matvec_XH_X, dtype=X.dtype, shape=(X.shape[1], X.shape[1]))\n",
      "        eigvals = scipy.sparse.linalg.eigs(XH_X, k=1, tol=0, maxiter=None, ncv=10, which=which, v0=v0, return_eigenvectors=False)\n",
      "        lsv = np.sqrt(eigvals)\n",
      "        return lsv[0].real\n",
      "\n",
      "    def matching_pursuit_sparse(self, y, X, feature_avg, k, tol=10**-10):\n",
      "        '''\n",
      "        Matching Pursuit\n",
      "        '''\n",
      "        r = y\n",
      "        X = X.tocsc()\n",
      "        err_norm = np.linalg.norm(r, 2)\n",
      "        err_norm_prev = 0\n",
      "        beta = np.zeros(X.shape[1])\n",
      "        while self.card(beta) < k:\n",
      "            all_inner_products = X.T.dot(r) - np.sum(r)*feature_avg\n",
      "            max_index, max_abs_inner_product = max(enumerate(np.abs(all_inner_products)), key=operator.itemgetter(1))\n",
      "            g = X[:, max_index]\n",
      "            g = np.squeeze(np.asarray(g.todense())) - feature_avg[max_index]\n",
      "            a = all_inner_products[max_index]\n",
      "            a /= np.linalg.norm(g, 2)**2\n",
      "            beta[max_index] += a\n",
      "            r = r - a*g\n",
      "            err_norm_prev = err_norm\n",
      "            err_norm = np.linalg.norm(r, 2)\n",
      "            if np.abs(err_norm - err_norm_prev) <= tol:\n",
      "                break\n",
      "        return beta\n",
      "\n",
      "    def thresholder(self, y,m):\n",
      "        sort_y = sorted(np.abs(y))\n",
      "        thresh = sort_y[-m]\n",
      "\n",
      "        non_thresholded_indices = (np.abs(y) > thresh)\n",
      "        n_nonzero_indices = sum(non_thresholded_indices)\n",
      "        if n_nonzero_indices < m:\n",
      "            collisions = np.where((np.abs(y)==thresh))[0]\n",
      "            passed = np.random.choice(collisions,m-n_nonzero_indices)\n",
      "            non_thresholded_indices[passed] = 1\n",
      "\n",
      "        y_new = non_thresholded_indices * y\n",
      "\n",
      "        return y_new, thresh\n",
      "\n",
      "    def AIHT_sparse(self, y, X, beta, k, feature_avg=None, alpha=0, example_weights=None, max_iters=10000, tol=10**-16):\n",
      "        \"\"\"Solves DORE accelerated IHT with a sparse matrix X.\n",
      "        \"\"\"\n",
      "        m, n = X.shape\n",
      "        y = np.squeeze(np.asarray(y))\n",
      "        err_norm_prev = 0\n",
      "        beta_0 = beta\n",
      "        beta_prev = beta\n",
      "        X_beta = 0\n",
      "        X_beta_prev = 0\n",
      "        X_beta_twice_prev = 0\n",
      "\n",
      "        if feature_avg is None:\n",
      "            feature_avg = np.zeros(n)\n",
      "    \n",
      "        if example_weights is None:\n",
      "            example_weights = np.ones(m)\n",
      "    \n",
      "        for iter_ in xrange(max_iters):\n",
      "            X_beta_twice_prev = X_beta_prev\n",
      "            X_beta_prev = X_beta\n",
      "            X_beta = (X.dot(beta) - feature_avg.dot(beta))\n",
      "            X_beta = np.squeeze(np.asarray(X_beta))\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta\n",
      "            norm_change = ((np.linalg.norm(beta - beta_prev)**2)/n)\n",
      "            print err.dot(err) + err_reg.dot(err_reg), norm_change, np.linalg.norm(beta)\n",
      "\n",
      "            if iter_ > 0 and (norm_change <= tol):\n",
      "                break\n",
      "\n",
      "            beta_t = beta + np.squeeze(np.asarray(X.T.dot(err))) - err.sum()*feature_avg + alpha*err_reg\n",
      "            beta_t = np.squeeze(np.asarray(beta_t))\n",
      "    \n",
      "            beta_t, thresh = self.thresholder(beta_t,k)\n",
      "            X_beta = X.dot(beta_t) - feature_avg.dot(beta_t)\n",
      "            X_beta = np.squeeze(X_beta)\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta_t\n",
      "    \n",
      "            beta_t_star = beta_t\n",
      "            if iter_ > 2:\n",
      "                delta_X_beta = X_beta - X_beta_prev\n",
      "                delta_regularization = alpha*(beta_t - beta)\n",
      "                dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                if dp > 0:\n",
      "                    a1 = (delta_X_beta.dot(err) + delta_regularization.dot(err_reg))/dp\n",
      "                    X_beta_1 = (1+a1)*X_beta - a1*X_beta_prev\n",
      "                    beta_1 = beta_t + a1*(beta_t - beta)\n",
      "                    err_1 = y - example_weights*X_beta_1\n",
      "                    err_1_reg = -alpha*beta_1\n",
      "    \n",
      "                    delta_X_beta = X_beta_1 - X_beta_twice_prev\n",
      "                    delta_regularization = alpha*(beta_1 - beta_prev)\n",
      "                    dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                    if dp > 0:\n",
      "                        a2 = (delta_X_beta.dot(err_1) + delta_regularization.dot(err_1_reg))/dp\n",
      "                        beta_2 = beta_1 + a2*(beta_1 - beta_prev)\n",
      "                        beta_2, thresh = self.thresholder(beta_2,k)\n",
      "    \n",
      "                        X_beta_2 = X.dot(beta_2) - feature_avg.dot(beta_2)\n",
      "                        X_beta_2 = np.squeeze(np.asarray(X_beta_2))\n",
      "                        err_2 = y - example_weights*X_beta_2\n",
      "                        err_reg_2 = -alpha*beta_2\n",
      "    \n",
      "                        if (err_2.dot(err_2) + err_reg_2.dot(err_reg_2)) / (err.dot(err) + err_reg.dot(err_reg)) < 1:\n",
      "                            beta_t_star = beta_2\n",
      "                            X_beta = X_beta_2\n",
      "    \n",
      "            beta_prev = beta\n",
      "            beta = beta_t_star\n",
      "    \n",
      "        return beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = IHTClassifier()\n",
      "sparse_matrix = scipy.sparse.csr_matrix(A)\n",
      "classifier.train(sparse_matrix, b, verbose=True)\n",
      "IHT_weights = classifier.beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################################################\n",
      "# Sorts words by their weights of the above computed portion                       #\n",
      "####################################################################################\n",
      "\n",
      "print(\"IHT Keywords\")\n",
      "all_terms = [(x, IHT_weights[i]) for i, x in enumerate(features)]\n",
      "all_terms.sort(key=lambda x: x[1], reverse=True)\n",
      "print([x[0] for x in all_terms[:20]])\n",
      "\n",
      "print(\"Lasso Keywords\")\n",
      "for weights in lasso_weights:\n",
      "    all_terms = [(x, weights[i]) for i, x in enumerate(features)]\n",
      "    all_terms.sort(key=lambda x: x[1], reverse=True)\n",
      "    print([x[0] for x in all_terms[:20]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}