{
 "metadata": {
  "name": "",
  "signature": "sha256:fa0718686d3866f93711c5e40404d111eb15948c923db42f16bc1ede8b5da24b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# topic_iterator.py\n",
      "\n",
      "import scipy.sparse\n",
      "import math\n",
      "import numpy\n",
      "\n",
      "class Options:\n",
      "    def __init__(self, threshold_m, threshold_n, max_iteration, tolerance, num_pc):\n",
      "        self.threshold_m = threshold_m\n",
      "        self.threshold_n = threshold_n\n",
      "        self.max_iteration = max_iteration\n",
      "        self.tolerance = tolerance\n",
      "        self.num_pc = num_pc\n",
      "\n",
      "class Iterator:\n",
      "    def __init__(self, options, M):\n",
      "        self.M = scipy.sparse.csc_matrix(M)\n",
      "        self.options = options\n",
      "    def thresh(self, s_vector, num_entries):\n",
      "        listOfValues = s_vector.T.todense().tolist()[0]\n",
      "        indices = self.sort(listOfValues)[1]\n",
      "        thresholded_vec = numpy.zeros(shape=s_vector.T.shape)\n",
      "        for i in range(num_entries):\n",
      "            index = indices[i]\n",
      "            thresholded_vec[0, index] = listOfValues[index]\n",
      "        return scipy.sparse.csr_matrix(thresholded_vec).T\n",
      "\n",
      "    def top_k_ind(self, s_vector, num_entries):\n",
      "        listOfValues = s_vector.T.todense().tolist()[0]\n",
      "        return self.sort(listOfValues)\n",
      "\n",
      "    def sort(self, vector):\n",
      "        indices = range(len(vector))\n",
      "        val_ind = map (lambda x : (abs(vector[x]),x), indices)\n",
      "        sorted_val_ind = sorted(val_ind)\n",
      "        sorted_val_ind.reverse()\n",
      "        sorted_indices = map (lambda x : x[1], sorted_val_ind)\n",
      "        sorted_values = map (lambda x : x[0], sorted_val_ind)\n",
      "        return (sorted_values, sorted_indices)\n",
      "    def single_iteration (self):\n",
      "\n",
      "        M = self.M\n",
      "        m = self.M.shape[0]\n",
      "        n = self.M.shape[1]\n",
      "        k_p = self.options.threshold_m\n",
      "        k_q = self.options.threshold_n\n",
      "        tolerance = self.options.tolerance\n",
      "        max_iteration = self.options.max_iteration\n",
      "\n",
      "\n",
      "        ini = self.ini_mat();\n",
      "        p = ini[0]\n",
      "        q = ini[1]\n",
      "        obj0 = float(\"inf\")\n",
      "        converged = False\n",
      "        iter = 0\n",
      "        while not converged:\n",
      "            p_new = self.thresh((M*q), k_p)\n",
      "            #print 'value is ' + str(sum(map(lambda x: x*x, p_new.T.todense().tolist()[0])))\n",
      "            #print 'max p is' + str(max(p_new.todense().tolist()[0]))\n",
      "            p = p_new\n",
      "            #print 'value is ' + str(p_new.todense())\n",
      "            #print 'q thresholding: '\n",
      "            q_new = self.thresh((p.T * M).T, k_q)\n",
      "            q = q_new/sum(map(abs,(q_new.data)))\n",
      "            q_test = (p.T * M)\n",
      "            #print 'max qt is' + str((p.T * M).T.shape)\n",
      "            #print 'max q is' + str(max(q_new.todense().tolist()[0]))\n",
      "            updated_mat = M-p_new*(q_new.T)\n",
      "            obj1 = updated_mat.data.dot(updated_mat.data)\n",
      "            if (abs(obj1-obj0) <= tolerance or iter >= max_iteration):\n",
      "                converged = 1\n",
      "            iter = iter + 1\n",
      "            obj0 = obj1\n",
      "        return (p,q)\n",
      "\n",
      "    def multiple_iterations (self):\n",
      "        term_inds = []\n",
      "        doc_inds = []\n",
      "        term_vals = []\n",
      "        doc_vals = []\n",
      "        for i in range(self.options.num_pc):\n",
      "            print 'handling pc# ' + str(i)\n",
      "            (p,q) = self.single_iteration()\n",
      "            (p_val, p_ind) = self.top_k_ind(p, self.options.threshold_m)\n",
      "            (q_val, q_ind) = self.top_k_ind(q, self.options.threshold_n)\n",
      "            doc_inds.append(p_ind[0:self.options.threshold_m])\n",
      "            term_inds.append(q_ind[0:self.options.threshold_n])\n",
      "            doc_vals.append(p_val[0:self.options.threshold_m])\n",
      "            term_vals.append(q_val[0:self.options.threshold_n])\n",
      "            self.remove_cols(q_ind[0:self.options.threshold_n])\n",
      "            self.remove_rows(p_ind[0:self.options.threshold_m])\n",
      "\n",
      "        return ((doc_vals,doc_inds),(term_vals,term_inds))\n",
      "\n",
      "    @staticmethod\n",
      "    def run(matrix, dict, titlelist, categorylist, output_file_name):\n",
      "        print 'running'\n",
      "\n",
      "        opt = Options(5, 5, 50, 0.0001, 3)\n",
      "        it = Iterator(opt, matrix)\n",
      "        word_pcs = []\n",
      "        title_pcs = []\n",
      "\n",
      "        dwords='var dwords=['\n",
      "        fnames = 'var fnames=['\n",
      "        links = 'var links=['\n",
      "        values = 'var values=['\n",
      "        fvalues = 'var fvalues=['\n",
      "\n",
      "        ((p_vals,p_inds),(q_vals,q_inds)) = it.multiple_iterations()\n",
      "        for pc, pcv in zip(q_inds, q_vals):\n",
      "            word_pc = []\n",
      "            dwordstr = '['\n",
      "            valstr = '['\n",
      "            for q_ind, q_val in zip(pc, pcv):\n",
      "                word_pc.append('%.4f\\t%s' % (q_val, dict[q_ind]))\n",
      "                dwordstr = dwordstr + format('\"%s\",' %(dict[q_ind]))\n",
      "                valstr = valstr + format('%.3f,' %(q_val))\n",
      "            dwordstr = dwordstr[0:len(dwordstr)-1] + ']'\n",
      "            valstr = valstr[0:len(valstr)-1] + ']'\n",
      "            dwords = dwords+dwordstr+','\n",
      "            values = values + valstr+','\n",
      "\n",
      "            word_pcs.append(word_pc)\n",
      "            todel = sorted(pc);\n",
      "            todel.reverse();\n",
      "            for q_ind in todel:\n",
      "                del dict[q_ind]\n",
      "        for pc, pcv in zip(p_inds, p_vals):\n",
      "            title_pc = []\n",
      "            fnamestr = '['\n",
      "            fvalstr = '['\n",
      "            linkstr = '['\n",
      "            for p_ind,p_val in zip(pc,pcv):\n",
      "                title_pc.append('%.4f\\t%s\\t%s' % (p_val,titlelist[p_ind],categorylist[p_ind]))\n",
      "                fnamestr = fnamestr + format('\"%s\",' %(titlelist[p_ind]))\n",
      "                linkstr = linkstr + format('\"%s\",' %(categorylist[p_ind]))\n",
      "                fvalstr = fvalstr + format('%.3f,' %(p_val))\n",
      "            fnamestr = fnamestr[0:len(fnamestr)-1] + ']'\n",
      "            fvalstr = fvalstr[0:len(fvalstr)-1] + ']'\n",
      "            linkstr = linkstr[0:len(linkstr)-1] + ']'\n",
      "\n",
      "            fnames = fnames + fnamestr+','\n",
      "            fvalues = fvalues + fvalstr + ','\n",
      "            links = links + linkstr + ',';\n",
      "            title_pcs.append(title_pc)\n",
      "            todel = sorted(pc);\n",
      "            todel.reverse();\n",
      "            for p_ind in todel:\n",
      "                del titlelist[p_ind]\n",
      "                del categorylist[p_ind]\n",
      "        dwords=dwords[0:len(dwords)-1]+'];';\n",
      "        fnames = fnames[0:len(fnames)-1]+'];';\n",
      "        links = links[0:len(links)-1]+'];';\n",
      "        values = values[0:len(values)-1]+'];';\n",
      "        fvalues = fvalues[0:len(fvalues)-1]+'];';\n",
      "        outputfile = open(output_file_name,'w')\n",
      "        outputfile.write('%s\\n%s\\n%s\\n%s\\n%s\\n'%(dwords, fnames, links, values, fvalues))\n",
      "        print 'Final results:'\n",
      "        print (dwords, fnames, links, values, fvalues)\n",
      "\n",
      "    def remove_rows(self, inds_to_remove):\n",
      "        self.M = self.M.T\n",
      "        self.remove_cols(inds_to_remove)\n",
      "        self.M = self.M.T\n",
      "    def remove_cols(self, inds_to_remove):\n",
      "        inds_to_remove = sorted(inds_to_remove)\n",
      "        inds_to_remove.reverse()\n",
      "        for ind in inds_to_remove:\n",
      "            self.remove_col(ind)\n",
      "\n",
      "    def remove_col(self, ind):\n",
      "        if ind == 0:\n",
      "            self.M = self.M[:,ind+1:].tocsc()\n",
      "        elif ind == (self.M.shape[1]-1):\n",
      "            self.M = self.M[:,0:ind].tocsc()\n",
      "        else:\n",
      "            self.M = scipy.sparse.hstack([self.M[:,0:ind],self.M[:,ind+1:]]).tocsc()\n",
      "\n",
      "    def ini_mat(self):\n",
      "        p = scipy.sparse.csc_matrix(numpy.ones(shape=(self.M.shape[0],1)))\n",
      "        q = scipy.sparse.csc_matrix(numpy.ones(shape=(self.M.shape[1],1)))\n",
      "        p = p/math.sqrt(p.T.dot(p).data[0])\n",
      "        q = q/math.sqrt(q.T.dot(q).data[0])\n",
      "        return (p,q)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# topic_summarizer.py\n",
      "import urllib2\n",
      "import csv\n",
      "import re\n",
      "import numpy as np\n",
      "import scipy\n",
      "from cookielib import CookieJar\n",
      "from bs4 import BeautifulSoup\n",
      "'''\n",
      "INPUT PARAMETERS\n",
      "'''\n",
      "cj = CookieJar()\n",
      "uri = \"http://export.arxiv.org/rss/cs\"\n",
      "\n",
      "class RSS_item():\n",
      "  def __init__(self, eltdict, all_docs_words):\n",
      "    self.title = eltdict['title']\n",
      "    self.source= eltdict['source']\n",
      "    self.content=eltdict['content'].lower().replace('\\n',' ')\n",
      "    self.content = re.sub(r'[^a-z]', ' ', self.content)\n",
      "    self.all_docs_words = all_docs_words\n",
      "    #print self.content\n",
      "    self.all_words={}\n",
      "\n",
      "    stopWords = set()\n",
      "    cr = csv.reader(open('./stopwords.txt'))\n",
      "    words=[]\n",
      "    for word in cr:\n",
      "     stopWords.add(word[0])\n",
      "\n",
      "    for word in nltk.word_tokenize(self.content):\n",
      "      if word not in stopWords:\n",
      "        self.all_docs_words.add(word)\n",
      "        if self.all_words.has_key(word):\n",
      "          self.all_words[word]=self.all_words[word] + 1\n",
      "        else:\n",
      "          self.all_words[word]=1\n",
      "\n",
      "def create_sparse_matrix_data(all_docs_words, all_docs):\n",
      "  rows, cols, entries = [], [], []\n",
      "  for document_index, doc in enumerate(all_docs):\n",
      "    for word in doc.all_words:\n",
      "      if doc.all_words[word] != 0:\n",
      "        rows.append(document_index)\n",
      "        cols.append(all_docs_words.index(word))\n",
      "        entries.append(doc.all_words[word])\n",
      "  rows, cols, entries = np.array(rows), np.array(cols), np.array(entries)\n",
      "  return scipy.sparse.csc_matrix((entries, (rows, cols)))\n",
      "\n",
      "def extract_data(uri):\n",
      "  all_docs_words = set()\n",
      "  opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n",
      "  html = opener.open(uri).read()\n",
      "  dicts = [];\n",
      "  all_docs = []\n",
      "  soup = BeautifulSoup(html,'xml')\n",
      "  for elt in soup.findAll('item'):\n",
      "    titlestr = elt.find('title').getText()\n",
      "    linkstr = elt.find('link').getText()\n",
      "    descstr = elt.find('description').getText()\n",
      "    eltdict = {};\n",
      "    eltdict['title']=titlestr.encode('utf-8')\n",
      "    eltdict['content']=descstr.encode('utf-8')\n",
      "    eltdict['source']=linkstr.encode('utf-8')\n",
      "    dicts.append(eltdict)\n",
      "\n",
      "  for doc in dicts:\n",
      "    all_docs.append(RSS_item(doc, all_docs_words))\n",
      "  all_docs_words=list(all_docs_words)\n",
      "\n",
      "  # Get links\n",
      "  link_strings = [doc['source'] for doc in dicts]\n",
      "\n",
      "  # Get document titles\n",
      "  doc_titles = [doc['title'] for doc in dicts]\n",
      "\n",
      "  # Create wordlist\n",
      "  wordlist = [word for word in all_docs_words]\n",
      "\n",
      "  # Create the sparse matrix data\n",
      "  sparse_matrix_data = create_sparse_matrix_data(all_docs_words, all_docs)\n",
      "  return sparse_matrix_data, wordlist, doc_titles, link_strings\n",
      "\n",
      "def summarize_text():\n",
      "  sparse_matrix_data, wordlist, doc_titles, link_strings = extract_data(uri)\n",
      "  Iterator.run(sparse_matrix_data, wordlist,\n",
      "               doc_titles, link_strings, 'output.js')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "summarize_text()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'sparse'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-55670e42374b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-1-690c3669b45f>\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0msparse_matrix_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_titles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   Iterator.run(sparse_matrix_data, wordlist,\n\u001b[1;32m     87\u001b[0m                doc_titles, link_strings, 'output.js')\n",
        "\u001b[0;32m<ipython-input-1-690c3669b45f>\u001b[0m in \u001b[0;36mextract_data\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0;31m# Create the sparse matrix data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0msparse_matrix_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sparse_matrix_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_docs_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msparse_matrix_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_titles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-1-690c3669b45f>\u001b[0m in \u001b[0;36mcreate_sparse_matrix_data\u001b[0;34m(all_docs_words, all_docs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mentries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsc_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'sparse'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}