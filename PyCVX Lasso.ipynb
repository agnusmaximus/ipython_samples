{
 "metadata": {
  "name": "",
  "signature": "sha256:b3a18a85239e758c2a3a565c1cf511a40120792067f320c179ae8f6ce08ec135"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################################################\n",
      "# Creates a term document matrix from movie reviews data          #\n",
      "###################################################################\n",
      "import sys\n",
      "import os\n",
      "import glob\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "START_K = 0\n",
      "K = 200\n",
      "\n",
      "# Movie data corpus\n",
      "negative_files = glob.glob(\"movie_review_data/neg/*.txt\")\n",
      "positive_files = glob.glob(\"movie_review_data/pos/*.txt\")\n",
      "all_file_names = negative_files + positive_files\n",
      "\n",
      "# Load valid english words\n",
      "valid_words = set()\n",
      "dictionary_file = open(\"words\")\n",
      "for word in dictionary_file:\n",
      "    word = word.lower().strip()\n",
      "    valid_words.add(word)\n",
      "    \n",
      "# Load english stop words\n",
      "stop_words = set()\n",
      "stop_words_file = open(\"stop_words\")\n",
      "for word in stop_words_file:\n",
      "    word = word.lower().strip()\n",
      "    stop_words.add(word)\n",
      "\n",
      "# Filter even more. Only take the K top most frequent words from the combined corpus\n",
      "word_frequencies = {}\n",
      "corpus_words = set()\n",
      "for file_name in all_file_names:\n",
      "    f = open(file_name)\n",
      "    text = unicode(f.read(), 'latin-1')\n",
      "    for word in text.split(\" \"):\n",
      "        word = word.lower().strip()\n",
      "        if word in valid_words and word not in stop_words:\n",
      "            if word not in word_frequencies:\n",
      "                word_frequencies[word] = 0\n",
      "            word_frequencies[word] += 1\n",
      "            corpus_words.add(word)\n",
      "    f.close()\n",
      "valid_features = sorted(corpus_words, key=lambda x: word_frequencies[x], reverse=True)[START_K:START_K+K]\n",
      "                \n",
      "# Collect all movie review text (both positive and negatively sentimented)\n",
      "corpus = []\n",
      "for count, file_name in enumerate(all_file_names):\n",
      "    f = open(file_name)\n",
      "    document_string = unicode(f.read(), 'latin-1')\n",
      "    filtered_document_string = \"\"\n",
      "    for word in document_string.split(\" \"):\n",
      "        word = word.lower().strip()\n",
      "        if word in valid_features:\n",
      "            filtered_document_string += word + ' '\n",
      "    corpus.append(filtered_document_string)\n",
      "    f.close()\n",
      "print(\"Done creating corpus!\")\n",
      "\n",
      "# Use Sklearn to get the term document matrix\n",
      "vectorizer = TfidfVectorizer(min_df=1, stop_words=\"english\")\n",
      "term_doc_matrix = vectorizer.fit_transform(corpus)\n",
      "\n",
      "# tdm and features\n",
      "tdm = term_doc_matrix.toarray()\n",
      "features = vectorizer.get_feature_names()\n",
      "\n",
      "print(\"Number of rows: %d cols: %d\" % tdm.shape)\n",
      "print(tdm)\n",
      "print(\"Features:\")\n",
      "print(features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done creating corpus!\n",
        "Number of rows: 1400 cols: 191"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[[ 0.          0.09493509  0.09173926 ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.07059803 ...,  0.          0.06947842  0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " ..., \n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
        " [ 0.08754548  0.          0.06867588 ...,  0.08877089  0.          0.        ]]\n",
        "Features:\n",
        "[u'able', u'acting', u'action', u'actor', u'actually', u'alien', u'american', u'audience', u'bad', u'based', u'believe', u'bit', u'black', u'book', u'boring', u'boy', u'camera', u'car', u'care', u'cast', u'character', u'city', u'comedy', u'comes', u'comic', u'completely', u'couple', u'course', u'dark', u'daughter', u'david', u'day', u'days', u'dead', u'death', u'deep', u'despite', u'dialogue', u'directed', u'direction', u'director', u'doing', u'drama', u'effects', u'entertaining', u'entire', u'especially', u'evil', u'exactly', u'example', u'family', u'father', u'feel', u'film', u'final', u'finally', u'friend', u'fun', u'funny', u'game', u'getting', u'girl', u'goes', u'guy', u'half', u'hand', u'hard', u'head', u'help', u'hit', u'hollywood', u'home', u'horror', u'hour', u'house', u'human', u'humor', u'idea', u'instead', u'jack', u'james', u'job', u'joe', u'john', u'kevin', u'kids', u'language', u'lee', u'left', u'life', u'line', u'little', u'live', u'look', u'looking', u'lost', u'lot', u'love', u'main', u'matter', u'maybe', u'michael', u'mind', u'moment', u'money', u'mother', u'movie', u'music', u'nearly', u'nice', u'night', u'novel', u'obvious', u'original', u'past', u'paul', u'people', u'perfect', u'performance', u'person', u'peter', u'picture', u'play', u'plot', u'pretty', u'probably', u'production', u'rated', u'real', u'reason', u'relationship', u'rest', u'review', u'robert', u'role', u'run', u'running', u'scene', u'school', u'scream', u'screen', u'screenplay', u'script', u'seeing', u'seen', u'sense', u'sequence', u'series', u'set', u'sex', u'short', u'shot', u'simply', u'son', u'soon', u'special', u'star', u'starring', u'start', u'story', u'strong', u'style', u'summer', u'supporting', u'supposed', u'tell', u'time', u'times', u'title', u'tom', u'town', u'true', u'truly', u'try', u'trying', u'unfortunately', u'van', u'video', u'violence', u'visit', u'war', u'watch', u'watching', u'white', u'wife', u'woman', u'world', u'worst', u'worth', u'written', u'wrong']\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################################################\n",
      "# Searches query term within list of corpus                       #\n",
      "###################################################################\n",
      "\n",
      "query = unicode('action')\n",
      "query_presence_indicators = []\n",
      "\n",
      "# Create the indicator column\n",
      "if query in features:\n",
      "    location_in_matrix = features.index(query)\n",
      "    features.remove(query)\n",
      "    \n",
      "    # Remove column with query term from tdm\n",
      "    tdm = np.delete(tdm, location_in_matrix, axis=1)\n",
      "    \n",
      "    # Construct indicator vector\n",
      "    for document in tdm:\n",
      "        if document[location_in_matrix] != 0:\n",
      "            query_presence_indicators.append(1)\n",
      "        else:\n",
      "            query_presence_indicators.append(-1)\n",
      "else:\n",
      "    print(\"Query not found\")\n",
      "\n",
      "print(\"Number of documents\", len(query_presence_indicators))\n",
      "print(\"Number of times query appears in all documents: \", sum([1 if x == 1 else 0 for x in query_presence_indicators]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Number of documents', 1400)\n",
        "('Number of times query appears in all documents: ', 257)\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from cvxpy import *\n",
      "import numpy as np\n",
      "import cvxopt\n",
      "from multiprocessing import Pool\n",
      "\n",
      "# Problem data.\n",
      "A = tdm\n",
      "b = query_presence_indicators\n",
      "gamma = Parameter(sign=\"positive\")\n",
      "\n",
      "# Construct the problem.\n",
      "x = Variable(len(A[0]))\n",
      "objective = Minimize(sum_squares(A*x - b) + gamma*norm(x, 1))\n",
      "p = Problem(objective)\n",
      "\n",
      "# Assign a value to gamma and find the optimal x.\n",
      "def get_x(gamma_value):\n",
      "    gamma.value = gamma_value\n",
      "    result = p.solve()\n",
      "    return x.value\n",
      "\n",
      "gammas = np.logspace(-1, 2, num=100)\n",
      "\n",
      "# Parallel computation.\n",
      "print(\"Starting computation\")\n",
      "pool = Pool(processes=1)\n",
      "x_values = pool.map(get_x, gammas)\n",
      "\n",
      "# Serial computation.\n",
      "lasso_weights = [get_x(value) for value in gammas]\n",
      "\n",
      "for v1,v2 in zip(x_values, lasso_weights):\n",
      "    if np.linalg.norm(v1 - v2) > 1e-5:\n",
      "        print \"error\"\n",
      "        \n",
      "print(\"Done!\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Starting computation\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done!\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Copyright (c) 2014 Steve Yadlowsky, Preetum Nakkarin.\n",
      "# Licensed under MIT License.\n",
      "# More information including the exact terms of the License\n",
      "# can be found in the file COPYING in the project root directory.\n",
      "\n",
      "import numpy as np\n",
      "import time\n",
      "import operator\n",
      "import scipy.sparse\n",
      "\n",
      "class IHTClassifier(object):\n",
      "    \n",
      "    def __init__(self):\n",
      "        self.training_time = 0.0\n",
      "        self.beta = None\n",
      "\n",
      "    def card(self, x):\n",
      "        return np.sum(x != 0)\n",
      "\n",
      "    def train(self, X, y, card=100, verbose=False):\n",
      "        start = time.time()\n",
      "        if verbose:\n",
      "            print \"Preconditioning matrix\"\n",
      "        whitened_X, feature_avg = self.whiten_features(X)\n",
      "        lsv = float(self.compute_lsv(whitened_X, feature_avg))\n",
      "        if verbose:\n",
      "            print \"Matching pursuits\"\n",
      "        x_hat = self.matching_pursuit_sparse(y, whitened_X/lsv, feature_avg/lsv, card)\n",
      "        if verbose:\n",
      "            print \"Running iterative hard thresholding\"\n",
      "        self.beta = self.AIHT_sparse(y, whitened_X/lsv, x_hat, card, feature_avg/lsv)/lsv\n",
      "\n",
      "        self.training_time += time.time() - start\n",
      "\n",
      "    def whiten_features(self, X):\n",
      "        X = X.tocsr(copy=True)\n",
      "        row_avg = np.bincount(X.indices, weights=X.data)\n",
      "        row_avg /= float(X.shape[0])\n",
      "        row_norm = np.bincount(X.indices, weights=(X.data - row_avg[X.indices])**2)\n",
      "        nonzeros_in_each_column = np.diff(X.tocsc().indptr)\n",
      "        avg_norm = ((float(X.shape[0])*np.ones(X.shape[1])) - nonzeros_in_each_column)*(row_avg**2)\n",
      "        row_norm += avg_norm\n",
      "        row_norm = np.array([np.sqrt(x) if x != 0 else 1 for x in row_norm])\n",
      "        row_avg /= row_norm\n",
      "        X.data /= np.take(row_norm, X.indices)\n",
      "        feature_avg = np.squeeze(row_avg)\n",
      "\n",
      "        return X, feature_avg\n",
      "\n",
      "    def compute_lsv(self, X, feature_avg):\n",
      "        def matmuldyad(v):\n",
      "            return X.dot(v) - feature_avg.dot(v)\n",
      "\n",
      "        def rmatmuldyad(v):\n",
      "            return X.T.dot(v) - v.sum()*feature_avg\n",
      "        normalized_lin_op = scipy.sparse.linalg.LinearOperator(X.shape, matmuldyad, rmatmuldyad)\n",
      "\n",
      "        def matvec_XH_X(v):\n",
      "            return normalized_lin_op.rmatvec(normalized_lin_op.matvec(v))\n",
      "\n",
      "        which='LM'\n",
      "        v0=None\n",
      "        maxiter=None\n",
      "        return_singular_vectors=False\n",
      "\n",
      "        XH_X = scipy.sparse.linalg.LinearOperator(matvec=matvec_XH_X, dtype=X.dtype, shape=(X.shape[1], X.shape[1]))\n",
      "        eigvals = scipy.sparse.linalg.eigs(XH_X, k=1, tol=0, maxiter=None, ncv=10, which=which, v0=v0, return_eigenvectors=False)\n",
      "        lsv = np.sqrt(eigvals)\n",
      "        return lsv[0].real\n",
      "\n",
      "    def matching_pursuit_sparse(self, y, X, feature_avg, k, tol=10**-10):\n",
      "        '''\n",
      "        Matching Pursuit\n",
      "        '''\n",
      "        r = y\n",
      "        X = X.tocsc()\n",
      "        err_norm = np.linalg.norm(r, 2)\n",
      "        err_norm_prev = 0\n",
      "        beta = np.zeros(X.shape[1])\n",
      "        while self.card(beta) < k:\n",
      "            all_inner_products = X.T.dot(r) - np.sum(r)*feature_avg\n",
      "            max_index, max_abs_inner_product = max(enumerate(np.abs(all_inner_products)), key=operator.itemgetter(1))\n",
      "            g = X[:, max_index]\n",
      "            g = np.squeeze(np.asarray(g.todense())) - feature_avg[max_index]\n",
      "            a = all_inner_products[max_index]\n",
      "            a /= np.linalg.norm(g, 2)**2\n",
      "            beta[max_index] += a\n",
      "            r = r - a*g\n",
      "            err_norm_prev = err_norm\n",
      "            err_norm = np.linalg.norm(r, 2)\n",
      "            if np.abs(err_norm - err_norm_prev) <= tol:\n",
      "                break\n",
      "        return beta\n",
      "\n",
      "    def thresholder(self, y,m):\n",
      "        sort_y = sorted(np.abs(y))\n",
      "        thresh = sort_y[-m]\n",
      "\n",
      "        non_thresholded_indices = (np.abs(y) > thresh)\n",
      "        n_nonzero_indices = sum(non_thresholded_indices)\n",
      "        if n_nonzero_indices < m:\n",
      "            collisions = np.where((np.abs(y)==thresh))[0]\n",
      "            passed = np.random.choice(collisions,m-n_nonzero_indices)\n",
      "            non_thresholded_indices[passed] = 1\n",
      "\n",
      "        y_new = non_thresholded_indices * y\n",
      "\n",
      "        return y_new, thresh\n",
      "\n",
      "    def AIHT_sparse(self, y, X, beta, k, feature_avg=None, alpha=0, example_weights=None, max_iters=10000, tol=10**-16):\n",
      "        \"\"\"Solves DORE accelerated IHT with a sparse matrix X.\n",
      "        \"\"\"\n",
      "        m, n = X.shape\n",
      "        y = np.squeeze(np.asarray(y))\n",
      "        err_norm_prev = 0\n",
      "        beta_0 = beta\n",
      "        beta_prev = beta\n",
      "        X_beta = 0\n",
      "        X_beta_prev = 0\n",
      "        X_beta_twice_prev = 0\n",
      "\n",
      "        if feature_avg is None:\n",
      "            feature_avg = np.zeros(n)\n",
      "    \n",
      "        if example_weights is None:\n",
      "            example_weights = np.ones(m)\n",
      "    \n",
      "        for iter_ in xrange(max_iters):\n",
      "            X_beta_twice_prev = X_beta_prev\n",
      "            X_beta_prev = X_beta\n",
      "            X_beta = (X.dot(beta) - feature_avg.dot(beta))\n",
      "            X_beta = np.squeeze(np.asarray(X_beta))\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta\n",
      "            norm_change = ((np.linalg.norm(beta - beta_prev)**2)/n)\n",
      "            print err.dot(err) + err_reg.dot(err_reg), norm_change, np.linalg.norm(beta)\n",
      "\n",
      "            if iter_ > 0 and (norm_change <= tol):\n",
      "                break\n",
      "\n",
      "            beta_t = beta + np.squeeze(np.asarray(X.T.dot(err))) - err.sum()*feature_avg + alpha*err_reg\n",
      "            beta_t = np.squeeze(np.asarray(beta_t))\n",
      "    \n",
      "            beta_t, thresh = self.thresholder(beta_t,k)\n",
      "            X_beta = X.dot(beta_t) - feature_avg.dot(beta_t)\n",
      "            X_beta = np.squeeze(X_beta)\n",
      "            err = y - example_weights*X_beta\n",
      "            err_reg = -alpha*beta_t\n",
      "    \n",
      "            beta_t_star = beta_t\n",
      "            if iter_ > 2:\n",
      "                delta_X_beta = X_beta - X_beta_prev\n",
      "                delta_regularization = alpha*(beta_t - beta)\n",
      "                dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                if dp > 0:\n",
      "                    a1 = (delta_X_beta.dot(err) + delta_regularization.dot(err_reg))/dp\n",
      "                    X_beta_1 = (1+a1)*X_beta - a1*X_beta_prev\n",
      "                    beta_1 = beta_t + a1*(beta_t - beta)\n",
      "                    err_1 = y - example_weights*X_beta_1\n",
      "                    err_1_reg = -alpha*beta_1\n",
      "    \n",
      "                    delta_X_beta = X_beta_1 - X_beta_twice_prev\n",
      "                    delta_regularization = alpha*(beta_1 - beta_prev)\n",
      "                    dp = delta_X_beta.dot(example_weights*delta_X_beta) + delta_regularization.dot(delta_regularization)\n",
      "                    if dp > 0:\n",
      "                        a2 = (delta_X_beta.dot(err_1) + delta_regularization.dot(err_1_reg))/dp\n",
      "                        beta_2 = beta_1 + a2*(beta_1 - beta_prev)\n",
      "                        beta_2, thresh = self.thresholder(beta_2,k)\n",
      "    \n",
      "                        X_beta_2 = X.dot(beta_2) - feature_avg.dot(beta_2)\n",
      "                        X_beta_2 = np.squeeze(np.asarray(X_beta_2))\n",
      "                        err_2 = y - example_weights*X_beta_2\n",
      "                        err_reg_2 = -alpha*beta_2\n",
      "    \n",
      "                        if (err_2.dot(err_2) + err_reg_2.dot(err_reg_2)) / (err.dot(err) + err_reg.dot(err_reg)) < 1:\n",
      "                            beta_t_star = beta_2\n",
      "                            X_beta = X_beta_2\n",
      "    \n",
      "            beta_prev = beta\n",
      "            beta = beta_t_star\n",
      "    \n",
      "        return beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classifier = IHTClassifier()\n",
      "sparse_matrix = scipy.sparse.csr_matrix(A)\n",
      "classifier.train(sparse_matrix, b, verbose=True)\n",
      "IHT_weights = classifier.beta"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Preconditioning matrix\n",
        "Matching pursuits\n",
        "Running iterative hard thresholding"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "771.669733839 0.0 41.9247021554\n",
        "771.342125832 0.00101102381147 41.9449606148\n",
        "771.170747965 0.000517712473581 41.9702252272\n",
        "771.073348771 0.000290281270755 41.9946853366\n",
        "770.923574569 0.00355801791594 42.0987683107\n",
        "770.913768411 0.000184010582394 42.1184737837\n",
        "770.91271039 1.8845093847e-05 42.1120230936\n",
        "770.91258154 2.1641415289e-06 42.1124337967\n",
        "770.91256659 2.17169715521e-07 42.1131317279\n",
        "770.91256438 3.53699412664e-08 42.11317276\n",
        "770.912564079 4.94785602995e-09 42.1133700928\n",
        "770.912564047 5.20305264648e-10 42.1133663954\n",
        "770.912564043 4.7264946299e-11 42.1133729687\n",
        "770.912564043 7.95568373342e-12 42.1133738431\n",
        "770.912564043 9.01193415526e-13 42.1133739697\n",
        "770.912564042 1.00765137595e-13 42.1133728257\n",
        "770.912564042 1.33727525303e-14 42.1133726196\n",
        "770.912564042"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.17531991265e-16 42.1133726042\n",
        "770.912564042 3.61565025085e-17 42.1133725975\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "####################################################################################\n",
      "# Sorts words by their weights of the above computed portion                       #\n",
      "####################################################################################\n",
      "\n",
      "print(\"IHT Keywords\")\n",
      "all_terms = [(x, IHT_weights[i]) for i, x in enumerate(features)]\n",
      "all_terms.sort(key=lambda x: x[1], reverse=True)\n",
      "print([x[0] for x in all_terms[:20]])\n",
      "\n",
      "print(\"Lasso Keywords\")\n",
      "for weights in lasso_weights:\n",
      "    all_terms = [(x, weights[i]) for i, x in enumerate(features)]\n",
      "    all_terms.sort(key=lambda x: x[1], reverse=True)\n",
      "    print([x[0] for x in all_terms[:20]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "IHT Keywords\n",
        "[u'actor', u'tom', u'film', u'novel', u'movie', u'original', u'acting', u'performance', u'character', u'jack', u'mind', u'shot', u'screenplay', u'comes', u'joe', u'seen', u'night', u'james', u'van', u'short']\n",
        "Lasso Keywords\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'acting', u'novel', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'short', u'mind', u'shot', u'screenplay', u'novel', u'acting', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'screenplay', u'novel', u'acting', u'character', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'novel', u'screenplay', u'character', u'acting', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'novel', u'screenplay', u'character', u'acting', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'worth', u'night']\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'night', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'goes', u'performance', u'sequence', u'strong', u'exactly', u'care', u'script', u'night', u'entire']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'performance', u'goes', u'sequence', u'strong', u'exactly', u'care', u'script', u'night', u'entire']\n",
        "[u'actor', u'rated', u'comes', u'mind', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'performance', u'goes', u'sequence', u'strong', u'care', u'exactly', u'script', u'night', u'entire']\n",
        "[u'actor', u'rated', u'mind', u'comes', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'performance', u'goes', u'sequence', u'strong', u'care', u'exactly', u'script', u'night', u'entire']\n",
        "[u'actor', u'rated', u'mind', u'comes', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'performance', u'goes', u'sequence', u'strong', u'care', u'exactly', u'script', u'night', u'entire']\n",
        "[u'actor', u'rated', u'mind', u'comes', u'short', u'shot', u'novel', u'character', u'screenplay', u'acting', u'instead', u'performance', u'goes', u'sequence', u'strong', u'script', u'exactly', u'care', u'night', u'entire']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'rated', u'mind', u'comes', u'short', u'novel', u'shot', u'character', u'screenplay', u'acting', u'instead', u'performance', u'goes', u'sequence', u'strong', u'script', u'exactly', u'care', u'night', u'entire']\n",
        "[u'actor', u'mind', u'rated', u'comes', u'short', u'novel', u'shot', u'character', u'screenplay', u'acting', u'instead', u'performance', u'goes', u'sequence', u'strong', u'script', u'exactly', u'care', u'night', u'entire']\n",
        "[u'actor', u'mind', u'rated', u'comes', u'short', u'novel', u'character', u'shot', u'screenplay', u'acting', u'instead', u'performance', u'sequence', u'goes', u'script', u'strong', u'exactly', u'care', u'night', u'entire']\n",
        "[u'actor', u'mind', u'rated', u'comes', u'short', u'novel', u'character', u'shot', u'screenplay', u'acting', u'instead', u'performance', u'sequence', u'goes', u'script', u'strong', u'exactly', u'care', u'night', u'entire']\n",
        "[u'actor', u'mind', u'comes', u'rated', u'short', u'novel', u'character', u'shot', u'screenplay', u'acting', u'performance', u'instead', u'sequence', u'goes', u'script', u'strong', u'exactly', u'care', u'night', u'entire']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'mind', u'comes', u'short', u'rated', u'novel', u'character', u'shot', u'screenplay', u'acting', u'performance', u'instead', u'sequence', u'goes', u'script', u'strong', u'exactly', u'care', u'night', u'entire']\n",
        "[u'actor', u'mind', u'comes', u'short', u'rated', u'novel', u'character', u'shot', u'screenplay', u'acting', u'performance', u'instead', u'sequence', u'goes', u'script', u'strong', u'night', u'exactly', u'care', u'entire']\n",
        "[u'actor', u'mind', u'comes', u'short', u'novel', u'rated', u'character', u'shot', u'acting', u'screenplay', u'performance', u'instead', u'sequence', u'goes', u'script', u'night', u'strong', u'exactly', u'care', u'tom']\n",
        "[u'actor', u'mind', u'comes', u'short', u'novel', u'rated', u'character', u'shot', u'acting', u'screenplay', u'performance', u'instead', u'sequence', u'goes', u'script', u'night', u'strong', u'exactly', u'care', u'tom']\n",
        "[u'actor', u'mind', u'novel', u'short', u'comes', u'character', u'rated', u'shot', u'acting', u'screenplay', u'performance', u'instead', u'sequence', u'goes', u'script', u'night', u'strong', u'exactly', u'care', u'tom']\n",
        "[u'actor', u'mind', u'novel', u'short', u'comes', u'character', u'rated', u'shot', u'acting', u'screenplay', u'performance', u'sequence', u'instead', u'goes', u'script', u'night', u'strong', u'exactly', u'care', u'tom']\n",
        "[u'actor', u'mind', u'novel', u'short', u'comes', u'character', u'rated', u'shot', u'acting', u'screenplay', u'performance', u'sequence', u'instead', u'goes', u'night', u'script', u'strong', u'tom', u'exactly', u'care']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'mind', u'novel', u'character', u'short', u'comes', u'rated', u'shot', u'acting', u'screenplay', u'performance', u'sequence', u'instead', u'goes', u'night', u'script', u'tom', u'strong', u'exactly', u'care']\n",
        "[u'actor', u'mind', u'novel', u'character', u'short', u'comes', u'rated', u'shot', u'acting', u'screenplay', u'performance', u'sequence', u'instead', u'goes', u'night', u'script', u'strong', u'exactly', u'tom', u'care']\n",
        "[u'actor', u'mind', u'novel', u'character', u'short', u'comes', u'shot', u'acting', u'rated', u'performance', u'screenplay', u'sequence', u'instead', u'goes', u'night', u'script', u'strong', u'exactly', u'entire', u'care']\n",
        "[u'actor', u'mind', u'novel', u'character', u'comes', u'short', u'performance', u'acting', u'shot', u'screenplay', u'rated', u'sequence', u'instead', u'goes', u'night', u'script', u'strong', u'exactly', u'entire', u'original']\n",
        "[u'actor', u'mind', u'novel', u'character', u'comes', u'short', u'performance', u'acting', u'shot', u'screenplay', u'rated', u'sequence', u'instead', u'goes', u'night', u'strong', u'script', u'exactly', u'entire', u'original']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'mind', u'character', u'comes', u'short', u'performance', u'acting', u'shot', u'screenplay', u'sequence', u'rated', u'instead', u'goes', u'night', u'strong', u'entire', u'original', u'script', u'exactly']\n",
        "[u'actor', u'novel', u'mind', u'character', u'comes', u'short', u'performance', u'acting', u'screenplay', u'shot', u'sequence', u'rated', u'instead', u'night', u'goes', u'strong', u'entire', u'original', u'exactly', u'script']\n",
        "[u'actor', u'novel', u'mind', u'character', u'short', u'comes', u'performance', u'acting', u'screenplay', u'shot', u'sequence', u'rated', u'instead', u'night', u'goes', u'entire', u'original', u'strong', u'exactly', u'care']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'acting', u'shot', u'screenplay', u'sequence', u'rated', u'instead', u'night', u'goes', u'original', u'strong', u'entire', u'exactly', u'care']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'acting', u'sequence', u'shot', u'screenplay', u'rated', u'instead', u'night', u'original', u'strong', u'entire', u'goes', u'exactly', u'care']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'acting', u'sequence', u'shot', u'screenplay', u'rated', u'instead', u'night', u'original', u'strong', u'entire', u'exactly', u'goes', u'care']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'acting', u'sequence', u'shot', u'screenplay', u'rated', u'instead', u'night', u'strong', u'original', u'exactly', u'entire', u'goes', u'care']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'acting', u'sequence', u'shot', u'screenplay', u'rated', u'instead', u'strong', u'exactly', u'night', u'original', u'entire', u'care', u'goes']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'sequence', u'acting', u'shot', u'screenplay', u'rated', u'instead', u'strong', u'exactly', u'night', u'original', u'entire', u'care', u'supporting']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'sequence', u'acting', u'shot', u'screenplay', u'rated', u'instead', u'strong', u'exactly', u'entire', u'original', u'night', u'care', u'supporting']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'sequence', u'acting', u'shot', u'rated', u'screenplay', u'instead', u'strong', u'exactly', u'entire', u'original', u'night', u'care', u'supporting']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'comes', u'sequence', u'acting', u'shot', u'rated', u'screenplay', u'strong', u'instead', u'exactly', u'entire', u'original', u'night', u'care', u'supporting']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'sequence', u'comes', u'acting', u'shot', u'rated', u'screenplay', u'strong', u'exactly', u'instead', u'entire', u'original', u'night', u'supporting', u'care']\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'sequence', u'comes', u'shot', u'acting', u'rated', u'screenplay', u'strong', u'exactly', u'instead', u'entire', u'original', u'night', u'supporting', u'care']\n",
        "[u'actor', u'novel', u'mind', u'character', u'performance', u'short', u'sequence', u'shot', u'acting', u'comes', u'rated', u'screenplay', u'strong', u'exactly', u'entire', u'instead', u'night', u'original', u'supporting', u'care']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'character', u'mind', u'performance', u'short', u'sequence', u'shot', u'acting', u'comes', u'rated', u'exactly', u'screenplay', u'strong', u'entire', u'supporting', u'instead', u'care', u'original', u'night']\n",
        "[u'actor', u'novel', u'mind', u'character', u'performance', u'short', u'sequence', u'shot', u'acting', u'comes', u'rated', u'exactly', u'strong', u'screenplay', u'entire', u'supporting', u'care', u'night', u'original', u'instead']\n",
        "[u'actor', u'novel', u'mind', u'character', u'performance', u'short', u'sequence', u'shot', u'acting', u'comes', u'rated', u'exactly', u'strong', u'screenplay', u'entire', u'supporting', u'care', u'style', u'night', u'original']\n",
        "[u'actor', u'novel', u'mind', u'character', u'performance', u'short', u'sequence', u'shot', u'acting', u'rated', u'comes', u'exactly', u'strong', u'entire', u'supporting', u'style', u'screenplay', u'care', u'believe', u'night']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'mind', u'character', u'performance', u'short', u'sequence', u'shot', u'acting', u'exactly', u'rated', u'comes', u'strong', u'entire', u'style', u'supporting', u'care', u'believe', u'screenplay', u'night']\n",
        "[u'actor', u'novel', u'mind', u'character', u'performance', u'short', u'sequence', u'shot', u'exactly', u'acting', u'rated', u'strong', u'comes', u'entire', u'style', u'supporting', u'believe', u'care', u'night', u'screenplay']\n",
        "[u'actor', u'novel', u'mind', u'character', u'short', u'performance', u'sequence', u'shot', u'exactly', u'acting', u'rated', u'strong', u'entire', u'style', u'comes', u'supporting', u'believe', u'care', u'jack', u'peter']\n",
        "[u'actor', u'novel', u'mind', u'character', u'short', u'performance', u'sequence', u'exactly', u'shot', u'acting', u'strong', u'rated', u'entire', u'style', u'supporting', u'believe', u'care', u'comes', u'jack', u'peter']\n",
        "[u'actor', u'novel', u'mind', u'character', u'short', u'performance', u'sequence', u'exactly', u'shot', u'strong', u'acting', u'entire', u'style', u'rated', u'supporting', u'believe', u'care', u'comes', u'peter', u'night']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'mind', u'character', u'short', u'exactly', u'shot', u'sequence', u'performance', u'strong', u'style', u'acting', u'entire', u'believe', u'supporting', u'rated', u'care', u'jack', u'peter', u'try']\n",
        "[u'actor', u'novel', u'mind', u'character', u'short', u'exactly', u'shot', u'sequence', u'performance', u'strong', u'style', u'entire', u'believe', u'acting', u'supporting', u'care', u'rated', u'jack', u'try', u'peter']\n",
        "[u'actor', u'novel', u'mind', u'character', u'exactly', u'short', u'shot', u'sequence', u'performance', u'style', u'strong', u'believe', u'entire', u'supporting', u'acting', u'care', u'rated', u'jack', u'completely', u'try']\n",
        "[u'actor', u'novel', u'mind', u'character', u'exactly', u'short', u'shot', u'sequence', u'style', u'strong', u'believe', u'performance', u'entire', u'supporting', u'acting', u'care', u'jack', u'completely', u'try', u'nearly']\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'character', u'short', u'shot', u'style', u'strong', u'believe', u'sequence', u'entire', u'performance', u'supporting', u'care', u'acting', u'completely', u'jack', u'try', u'nearly']\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'character', u'short', u'shot', u'style', u'believe', u'strong', u'sequence', u'entire', u'supporting', u'performance', u'care', u'completely', u'acting', u'jack', u'try', u'nearly']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'character', u'short', u'shot', u'style', u'strong', u'believe', u'sequence', u'entire', u'performance', u'supporting', u'acting', u'jack', u'nearly', u'completely', u'care', u'try']\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'character', u'short', u'believe', u'shot', u'style', u'strong', u'entire', u'sequence', u'supporting', u'completely', u'jack', u'acting', u'try', u'nearly', u'entertaining', u'reason']\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'believe', u'short', u'shot', u'style', u'character', u'strong', u'sequence', u'entire', u'supporting', u'nearly', u'jack', u'completely', u'care', u'reason', u'entertaining', u'performance']\n",
        "[u'actor', u'mind', u'novel', u'exactly', u'believe', u'short', u'shot', u'style', u'strong', u'character', u'sequence', u'entire', u'supporting', u'nearly', u'completely', u'reason', u'jack', u'entertaining', u'try', u'care']\n",
        "[u'actor', u'mind', u'novel', u'exactly', u'believe', u'shot', u'style', u'short', u'strong', u'character', u'sequence', u'entire', u'nearly', u'reason', u'completely', u'entertaining', u'supporting', u'jack', u'try', u'tell']\n",
        "[u'actor', u'mind', u'novel', u'exactly', u'believe', u'shot', u'style', u'short', u'strong', u'character', u'sequence', u'entire', u'reason', u'entertaining', u'nearly', u'supporting', u'completely', u'jack', u'care', u'tell']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'believe', u'shot', u'style', u'short', u'strong', u'character', u'sequence', u'entire', u'reason', u'entertaining', u'supporting', u'care', u'nearly', u'jack', u'try', u'tell']\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'believe', u'shot', u'style', u'short', u'strong', u'sequence', u'character', u'reason', u'entire', u'entertaining', u'care', u'nearly', u'jack', u'supporting', u'tell', u'try']\n",
        "[u'actor', u'novel', u'mind', u'exactly', u'believe', u'shot', u'style', u'strong', u'short', u'worst', u'sequence', u'reason', u'entertaining', u'entire', u'nearly', u'jack', u'care', u'character', u'supporting', u'tell']\n",
        "[u'actor', u'novel', u'mind', u'believe', u'exactly', u'shot', u'style', u'strong', u'short', u'worst', u'reason', u'sequence', u'entire', u'entertaining', u'nearly', u'jack', u'supporting', u'character', u'care', u'tell']\n",
        "[u'actor', u'novel', u'mind', u'believe', u'exactly', u'shot', u'style', u'strong', u'short', u'worst', u'reason', u'sequence', u'entire', u'supporting', u'entertaining', u'jack', u'nearly', u'care', u'tell', u'character']\n",
        "[u'actor', u'novel', u'mind', u'believe', u'exactly', u'shot', u'style', u'strong', u'short', u'worst', u'reason', u'sequence', u'supporting', u'entire', u'nearly', u'character', u'entertaining', u'jack', u'supposed', u'tell']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'mind', u'novel', u'believe', u'exactly', u'shot', u'style', u'strong', u'worst', u'short', u'reason', u'nearly', u'supposed', u'entire', u'entertaining', u'sequence', u'supporting', u'jack', u'care', u'tell']\n",
        "[u'actor', u'novel', u'mind', u'believe', u'exactly', u'shot', u'style', u'worst', u'strong', u'short', u'reason', u'sequence', u'supporting', u'entire', u'nearly', u'supposed', u'entertaining', u'jack', u'direction', u'tell']\n",
        "[u'actor', u'novel', u'mind', u'believe', u'exactly', u'shot', u'style', u'worst', u'strong', u'reason', u'short', u'sequence', u'entire', u'supporting', u'nearly', u'supposed', u'direction', u'entertaining', u'jack', u'tell']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'novel', u'mind', u'believe', u'exactly', u'shot', u'style', u'worst', u'strong', u'reason', u'supporting', u'short', u'sequence', u'entire', u'nearly', u'direction', u'supposed', u'jack', u'entertaining', u'tell']\n",
        "[u'actor', u'novel', u'mind', u'believe', u'exactly', u'shot', u'style', u'worst', u'strong', u'reason', u'supporting', u'short', u'sequence', u'direction', u'entire', u'nearly', u'supposed', u'entertaining', u'jack', u'tell']\n",
        "[u'actor', u'mind', u'novel', u'believe', u'exactly', u'shot', u'style', u'worst', u'supporting', u'strong', u'reason', u'short', u'sequence', u'entire', u'jack', u'supposed', u'nearly', u'direction', u'rest', u'example']\n",
        "[u'actor', u'believe', u'mind', u'exactly', u'novel', u'shot', u'worst', u'style', u'supporting', u'strong', u'reason', u'direction', u'supposed', u'entire', u'short', u'sequence', u'nearly', u'jack', u'rest', u'truly']\n",
        "[u'actor', u'believe', u'mind', u'novel', u'exactly', u'shot', u'worst', u'style', u'supporting', u'strong', u'reason', u'entire', u'supposed', u'direction', u'sequence', u'short', u'jack', u'nearly', u'truly', u'rest']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'believe', u'mind', u'exactly', u'novel', u'worst', u'shot', u'style', u'supporting', u'reason', u'strong', u'entire', u'direction', u'supposed', u'sequence', u'jack', u'nearly', u'short', u'truly', u'rest']\n",
        "[u'actor', u'believe', u'novel', u'mind', u'exactly', u'shot', u'worst', u'style', u'supporting', u'strong', u'reason', u'entire', u'jack', u'direction', u'supposed', u'sequence', u'short', u'truly', u'nearly', u'rest']\n",
        "[u'actor', u'novel', u'believe', u'mind', u'exactly', u'shot', u'supporting', u'style', u'worst', u'strong', u'performance', u'jack', u'sequence', u'character', u'reason', u'tom', u'short', u'entire', u'nearly', u'example']\n",
        "[u'actor', u'believe', u'novel', u'exactly', u'mind', u'worst', u'shot', u'style', u'supporting', u'strong', u'reason', u'jack', u'direction', u'entire', u'performance', u'supposed', u'truly', u'rest', u'example', u'nearly']\n",
        "[u'actor', u'believe', u'exactly', u'novel', u'worst', u'mind', u'shot', u'style', u'supporting', u'strong', u'reason', u'direction', u'jack', u'entire', u'supposed', u'truly', u'rest', u'example', u'nearly', u'title']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'believe', u'novel', u'exactly', u'mind', u'worst', u'shot', u'style', u'supporting', u'strong', u'reason', u'direction', u'jack', u'truly', u'entire', u'supposed', u'rest', u'example', u'nearly', u'title']\n",
        "[u'actor', u'believe', u'novel', u'supporting', u'exactly', u'mind', u'shot', u'style', u'worst', u'strong', u'direction', u'jack', u'performance', u'tom', u'reason', u'title', u'truly', u'rest', u'example', u'drama']\n",
        "[u'actor', u'believe', u'novel', u'mind', u'exactly', u'shot', u'supporting', u'worst', u'style', u'strong', u'jack', u'reason', u'performance', u'direction', u'tom', u'entire', u'truly', u'sequence', u'supposed', u'title']\n",
        "[u'actor', u'believe', u'novel', u'supporting', u'exactly', u'worst', u'mind', u'style', u'shot', u'strong', u'direction', u'jack', u'tom', u'title', u'performance', u'reason', u'truly', u'drama', u'rest', u'example']\n",
        "[u'actor', u'novel', u'character', u'believe', u'supporting', u'performance', u'mind', u'tom', u'strong', u'shot', u'style', u'exactly', u'robert', u'jack', u'peter', u'paul', u'short', u'sequence', u'direction', u'nearly']\n",
        "[u'actor', u'believe', u'novel', u'supporting', u'exactly', u'worst', u'style', u'shot', u'mind', u'strong', u'direction', u'tom', u'title', u'jack', u'drama', u'truly', u'paul', u'performance', u'reason', u'example']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[u'actor', u'believe', u'supporting', u'novel', u'exactly', u'style', u'worst', u'shot', u'mind', u'strong', u'direction', u'tom', u'paul', u'drama', u'jack', u'title', u'truly', u'example', u'car', u'able']\n",
        "[u'actor', u'believe', u'novel', u'supporting', u'exactly', u'style', u'shot', u'mind', u'strong', u'tom', u'paul', u'worst', u'direction', u'performance', u'jack', u'title', u'peter', u'drama', u'example', u'car']\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}